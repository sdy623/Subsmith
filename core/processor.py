"""
Main processing module - æ ¸å¿ƒå¤„ç†é€»è¾‘
"""

import json
import re
from pathlib import Path
from typing import List, Optional, Dict, Any
from dataclasses import asdict

try:
    import pandas as pd
except ImportError:
    pd = None

from .config import Config
from .card_data import CardData
from .media_handler import MediaHandler
from .subtitle_handler import SubtitleHandler
from .word_processor import WordProcessor
from .frequency import FrequencyIndex
from .csv_exporter import CSVExporter
from .anki_pusher import AnkiPusher
from .logger import ProcessLogger

from mdx_utils import MeaningsLookup, AudioLookup, get_all_audio_info_from_mdx


class MiningProcessor:
    """ä¸»å¤„ç†å™¨ - åè°ƒæ‰€æœ‰æ¨¡å—å®ŒæˆæŒ–çŸ¿ä»»åŠ¡"""
    
    def __init__(self, config: Config):
        self.config = config
        self.word_processor = WordProcessor()
        self.media_handler = MediaHandler()
        self.subtitle_handler = SubtitleHandler()
        self.logger = ProcessLogger(verbose=not config.quiet, quiet=config.quiet)
        
        # åˆå§‹åŒ–è¯å…¸æŸ¥è¯¢
        self.meanings_lookup = None
        self.audio_lookup = None
        self.freq_index = FrequencyIndex()
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å—"""
        print("\n" + "=" * 60)
        print("JP Media Mining (æ¨¡å—åŒ–ç‰ˆæœ¬)")
        print("=" * 60)
        
        # éªŒè¯é…ç½®
        errors = self.config.validate()
        if errors:
            print("\nâŒ é…ç½®é”™è¯¯:")
            for err in errors:
                print(f"   - {err}")
            return False
        
        # åŠ è½½é‡Šä¹‰æŸ¥è¯¢
        if self.config.primary_mdx or self.config.secondary_mdx or self.config.tertiary_mdx:
            print("\nğŸ“– åˆå§‹åŒ–é‡Šä¹‰æŸ¥è¯¢...")
            self.meanings_lookup = MeaningsLookup.from_dirs(
                primary_dir=self.config.primary_mdx,
                secondary_dir=self.config.secondary_mdx,
                tertiary_dir=self.config.tertiary_mdx,
                use_jamdict=self.config.use_jamdict
            )
            if self.meanings_lookup and self.meanings_lookup.all_dicts:
                print(f"   âœ… åŠ è½½è¯å…¸: {len(self.meanings_lookup.all_dicts)} ä¸ª")
        
        # åŠ è½½éŸ³é¢‘æŸ¥è¯¢
        if self.config.nhk_old or self.config.nhk_new or self.config.djs:
            print("\nğŸµ åˆå§‹åŒ–éŸ³é¢‘æŸ¥è¯¢...")
            self.audio_lookup = AudioLookup.from_dirs(
                nhk_old_dir=self.config.nhk_old,
                nhk_new_dir=self.config.nhk_new,
                djs_dir=self.config.djs
            )
            audio_count = len(self.audio_lookup.audio_dicts) if self.audio_lookup and self.audio_lookup.audio_dicts else 0
            print(f"   âœ… éŸ³é¢‘è¯å…¸: {audio_count} ä¸ª")
        
        # åŠ è½½é¢‘ç‡æ•°æ®
        if self.config.freq:
            print("\nğŸ“Š åŠ è½½é¢‘ç‡æ•°æ®...")
            self.freq_index = FrequencyIndex(self.config.freq)
            if self.freq_index.idx:
                print(f"   âœ… å·²åŠ è½½ {len(self.freq_index.idx)} æ¡é¢‘ç‡æ•°æ®")
        
        print("\nâœ… åˆå§‹åŒ–å®Œæˆ\n")
        return True
    
    def process(self) -> List[CardData]:
        """æ‰§è¡ŒæŒ–çŸ¿å¤„ç†æµç¨‹"""
        # åŠ è½½å­—å¹•
        print(f"ğŸ“„ åŠ è½½å­—å¹•: {self.config.subs.name}")
        subs = self.subtitle_handler.load_subs(self.config.subs)
        print(f"   âœ… å…± {len(subs)} è¡Œå­—å¹•")
        
        # åŠ è½½å•è¯
        print(f"\nğŸ“ åŠ è½½ç›®æ ‡å•è¯: {self.config.words.name}")
        words = self.word_processor.load_words(self.config.words)
        print(f"   âœ… å…± {len(words)} ä¸ªå•è¯")
        
        # æå–åª’ä½“ä¿¡æ¯
        print(f"\nğŸ“º æå–åª’ä½“ä¿¡æ¯...")
        anime_name, episode = self.subtitle_handler.extract_episode_info(
            self.config.video, self.config.words
        )
        print(f"   åŠ¨æ¼«å: {anime_name}")
        print(f"   é›†æ•°: {episode}")
        
        # æŸ¥æ‰¾åŒ¹é…
        print(f"\nğŸ” å¼€å§‹å¤„ç†å­—å¹•...")
        cards = self._find_hits(
            words=words,
            subs=subs,
            video=self.config.video,
            outdir=self.config.outdir,
            anime_name=anime_name,
            episode=episode,
            pad=self.config.pad,
            vf=self.config.vf,
            verbose=not self.config.quiet
        )
        
        print(f"\nâœ… å¤„ç†å®Œæˆ! å…±ç”Ÿæˆ {len(cards)} å¼ å¡ç‰‡")
        return cards
    
    def _find_hits(
        self,
        words: List[tuple],
        subs,
        video: Path,
        outdir: Path,
        anime_name: str,
        episode: str,
        pad: float,
        vf: Optional[str],
        verbose: bool
    ) -> List[CardData]:
        """æŸ¥æ‰¾å­—å¹•ä¸­çš„ç›®æ ‡å•è¯å¹¶ç”Ÿæˆå¡ç‰‡æ•°æ®"""
        MediaHandler.ensure_dir(outdir)
        cards: List[CardData] = []
        
        word_to_reading = {word: reading for word, reading, _ in words}
        word_to_lookup_form = {word: lookup_form for word, _, lookup_form in words}
        wset = set(word_to_reading.keys())
        
        # è¾“å‡ºå¤„ç†æ‘˜è¦
        self.logger.log_processing_summary(len(words), len(subs))
        
        for idx, line in enumerate(subs, 1):
            sent = self.subtitle_handler.normalize_sub_text(line.text)
            if not sent:
                continue
            
            # åˆ†è¯å¹¶æ£€æŸ¥åŒ¹é…
            lemmas = self.word_processor.lemmatize(sent)
            tokens_set = set(lemmas)
            
            matched_by_lemma = wset.intersection(tokens_set)
            matched_by_string = {word for word, _, _ in words if word in sent}
            matched = matched_by_lemma | matched_by_string
            
            if not matched:
                continue
            
            # è®°å½•å­—å¹•åŒ¹é…
            self.logger.log_subtitle_match(idx, len(subs), list(matched), sent)
            
            # ç”Ÿæˆå¸¦å‡åçš„å¥å­
            furig = self.word_processor.tokens_furigana(sent)
            
            # è®¡ç®—æ—¶é—´å’Œç”Ÿæˆåª’ä½“
            start = max(0.0, MediaHandler.ms_to_s(line.start) - pad)
            end = MediaHandler.ms_to_s(line.end) + pad
            mid = (start + end) / 2
            
            base = f"{video.stem}_{int(line.start)}_{int(line.end)}"
            img_path = outdir / f"{base}.jpg"
            aud_path = outdir / f"{base}.m4a"
            
            try:
                MediaHandler.screenshot(video, mid, img_path, vf)
                MediaHandler.cut_audio(video, start, end, aud_path)
            except Exception as e:
                self.logger.log_media_processing_error(e)
                continue
            
            # ä¸ºæ¯ä¸ªåŒ¹é…çš„å•è¯åˆ›å»ºå¡ç‰‡
            for word in matched:
                card = self._create_card(
                    word=word,
                    sent=sent,
                    furig=furig,
                    start=start,
                    end=end,
                    img_path=img_path,
                    aud_path=aud_path,
                    word_to_reading=word_to_reading,
                    word_to_lookup_form=word_to_lookup_form,
                    anime_name=anime_name,
                    episode=episode,
                    verbose=verbose
                )
                if card:
                    cards.append(card)
        
        return cards
    
    def _create_card(
        self,
        word: str,
        sent: str,
        furig: str,
        start: float,
        end: float,
        img_path: Path,
        aud_path: Path,
        word_to_reading: dict,
        word_to_lookup_form: dict,
        anime_name: str,
        episode: str,
        verbose: bool
    ) -> Optional[CardData]:
        """åˆ›å»ºå•å¼ å¡ç‰‡"""
        # è®°å½•æŸ¥è¯¢å¼€å§‹
        self.logger.log_word_query_start(word)
        
        # è·å–è¯å…ƒ
        word_lemma_parts = []
        for t in self.word_processor.tagger(word):
            part_lemma = None
            if hasattr(t.feature, 'lemma') and t.feature.lemma:
                part_lemma = t.feature.lemma
            elif hasattr(t, 'feature') and len(t.feature) > 6 and t.feature[6]:
                part_lemma = t.feature[6]
            else:
                part_lemma = t.surface
            if part_lemma:
                word_lemma_parts.append(part_lemma)
        
        word_lemma = ''.join(word_lemma_parts) if word_lemma_parts else word
        
        # è®°å½•è¯å…ƒå½¢å¼
        self.logger.log_lemma_form(word, word_lemma)
        
        # æ£€æŸ¥ç”¨æˆ·æŒ‡å®šæŸ¥è¯å½¢æ€
        user_lookup_form = word_to_lookup_form.get(word)
        if user_lookup_form:
            self.logger.log_user_lookup_form(user_lookup_form)
            word_lemma = user_lookup_form
        
        # å‡†å¤‡æŸ¥è¯¢å€™é€‰è¯
        is_original_katakana = WordProcessor.is_all_katakana(word)
        query_candidates = []
        
        if is_original_katakana and not user_lookup_form:
            query_candidates.append(word)
            if word_lemma != word:
                query_candidates.append(word_lemma)
        else:
            query_candidates.append(word_lemma)
            if word_lemma != word:
                query_candidates.append(word)
        
        if word_lemma.endswith('ã'):
            query_candidates.append(word_lemma[:-1] + 'ã')
        
        # æ£€æŸ¥å¼ºåˆ¶è¯»éŸ³
        forced_reading = word_to_reading.get(word)
        if forced_reading:
            self.logger.log_forced_reading(forced_reading)
        
        # 1. æŸ¥è¯¢é‡Šä¹‰
        definition = ""
        successful_query_form = word
        
        if self.meanings_lookup:
            try:
                if forced_reading:
                    definition = self.meanings_lookup.lookup(forced_reading)
                    if definition:
                        self.logger.log_definition_success(definition, "å‡åæŸ¥è¯¢")
                    else:
                        # Fallback åˆ°è¯å…ƒæŸ¥è¯¢
                        definition = self.meanings_lookup.lookup(word_lemma)
                        if definition:
                            successful_query_form = word_lemma
                            self.logger.log_definition_success(definition, "è¯å…ƒæŸ¥è¯¢")
                else:
                    for idx, candidate in enumerate(query_candidates):
                        definition = self.meanings_lookup.lookup(candidate)
                        if definition:
                            successful_query_form = candidate
                            if idx > 0:  # ä¸æ˜¯ç¬¬ä¸€ä¸ªå€™é€‰è¯
                                self.logger.log_variant_query(candidate)
                            self.logger.log_definition_success(definition)
                            break
                
                if not definition:
                    self.logger.log_definition_not_found()
            except Exception as e:
                self.logger.log_query_error("é‡Šä¹‰", e)
        
        # 2. æŸ¥è¯¢éŸ³é¢‘å’ŒéŸ³è°ƒ
        reading = ''
        pitch_pos = ''
        pitch_src = ''
        audio_src = ''
        all_readings_json = ''
        word_audio_b64 = ""
        audio_result = None
        
        if self.audio_lookup:
            try:
                if forced_reading:
                    audio_result = self.audio_lookup.lookup(forced_reading, verbose=False, return_all_pitches=True)
                else:
                    for idx, candidate in enumerate(query_candidates):
                        audio_result = self.audio_lookup.lookup(candidate, verbose=False, return_all_pitches=True)
                        if audio_result and audio_result.get('reading'):
                            if idx > 0:
                                self.logger.log_variant_query(candidate, "éŸ³é¢‘")
                            break
                
                if audio_result:
                    reading = audio_result.get('reading', '') or ''
                    pitch_pos = audio_result.get('pitch_position', '') or ''
                    pitch_src = audio_result.get('pitch_source', '') or ''
                    audio_src = audio_result.get('audio_source', '') or ''
                    
                    all_pitches = audio_result.get('all_pitches', [])
                    all_readings_json = json.dumps(
                        [{'reading': r, 'pitch_position': p} for r, p in all_pitches],
                        ensure_ascii=False
                    ) if all_pitches else ''
                    
                    if reading:
                        all_count = len(all_pitches) if all_pitches and len(all_pitches) > 1 else None
                        self.logger.log_reading_success(reading, pitch_pos, all_count)
                    
                    if audio_result.get('audio_base64'):
                        audio_b64 = audio_result['audio_base64']
                        audio_mime = audio_result.get('audio_mime', 'audio/mpeg')
                        word_audio_b64 = f"data:{audio_mime};base64,{audio_b64}"
                        self.logger.log_word_audio_success(audio_src)
                else:
                    self.logger.log_reading_not_found()
            except Exception as e:
                self.logger.log_query_error("éŸ³é¢‘", e)
        
        # 3. æŸ¥è¯¢é¢‘ç‡
        freq_str, freq_rank = None, None
        for idx, candidate in enumerate(query_candidates):
            freq_str, freq_rank = self.freq_index.lookup(candidate)
            if freq_str:
                if idx > 0:
                    self.logger.log_variant_query(candidate, "é¢‘ç‡")
                self.logger.log_frequency_success(freq_str, freq_rank)
                break
        
        if not freq_str:
            self.logger.log_frequency_not_found()
        
        # 4. éŸ³è°ƒç±»å‹
        pitch_type = self._pitch_position_to_type(pitch_pos, reading)
        if pitch_type:
            self.logger.log_pitch_type(pitch_type)
        
        # 5. Base64 ç¼–ç åª’ä½“
        self.logger.log_media_encoding()
        sentence_audio_b64 = MediaHandler.file_to_base64(aud_path)
        picture_b64 = MediaHandler.file_to_base64(img_path)
        
        return CardData(
            word=successful_query_form,
            sentence=sent,
            sentence_furigana=furig,
            definition=definition,
            reading=reading,
            pitch_position=pitch_pos,
            pitch_type=pitch_type,
            pitch_source=pitch_src,
            sentence_audio_base64=sentence_audio_b64,
            word_audio_base64=word_audio_b64,
            word_audio_source=audio_src,
            picture_base64=picture_b64,
            bccwj_frequency=freq_str or '',
            bccwj_freq_sort=str(freq_rank) if freq_rank is not None else '',
            anime_name=anime_name,
            episode=episode,
            start_time=start,
            end_time=end,
            lemma=word_lemma,
            all_readings=all_readings_json
        )
    
    @staticmethod
    def _pitch_position_to_type(pitch_position: str, reading: str = "") -> str:
        """å°†éŸ³è°ƒä½ç½®è½¬æ¢ä¸ºç±»å‹åç§°"""
        if not pitch_position:
            return ""
        match = re.search(r'\[(\d+)\]', pitch_position)
        if not match:
            return ""
        pos = int(match.group(1))
        
        if pos == 0:
            return "å¹³æ¿å¼"
        elif pos == 1:
            return "é ­é«˜å‹"
        else:
            if reading:
                clean_reading = re.sub(r'<[^>]+>', '', reading)
                mora_count = len(clean_reading)  # ç®€åŒ–å¤„ç†
                if mora_count > 0 and pos == mora_count:
                    return "å°¾é«˜å‹"
            return "ä¸­é«˜å‹"
