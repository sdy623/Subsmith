#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JP Media Mining â†’ Anki Cards (é‡æ„ç‰ˆ)

é‡æ„æ”¹è¿›:
1. ä½¿ç”¨ mdx_utils.MeaningsLookup è¿›è¡Œå¤šè¯å…¸é‡Šä¹‰æŸ¥è¯¢ (Yomitanæ ¼å¼)
2. ä½¿ç”¨ mdx_utils.AudioLookup è·å–éŸ³é¢‘å’ŒéŸ³è°ƒä¿¡æ¯ (æ”¯æŒ Fugashi æ™ºèƒ½åŒ¹é…)
3. ä¼˜åŒ–ä»£ç ç»“æ„,æé«˜å¯ç»´æŠ¤æ€§
4. é›†æˆ AnkiConnect API,ä¸€é”®æ¨é€åˆ° Anki

ä¾èµ–å®‰è£…:
  pip install pysubs2 fugashi[unidic-lite] requests pandas

ä½¿ç”¨ç¤ºä¾‹ (ç”Ÿæˆ CSV):
  python jp_media_mining_refactored.py \
    --video "episodes/Ep01.mp4" \
    --subs  "subs/Ep01.ja.srt" \
    --words "unknown/Ep01_words.txt" \
    --primary-mdx   "dicts/primary_mdx_dir" \
    --secondary-mdx "dicts/secondary_mdx_dir" \
    --tertiary-mdx  "dicts/tertiary_mdx_dir" \
    --nhk-old "dicts/NHK_Old" \
    --nhk-new "dicts/NHK_New" \
    --djs     "dicts/DJS" \
    --freq    "dicts/term_meta_bank_1.json" \
    --outdir  "out/Ep01" \
    --csv     "out/Ep01/cards.csv"

ä½¿ç”¨ç¤ºä¾‹ (ä¸€é”®æ¨é€åˆ° Anki):
  python jp_media_mining_refactored.py \
    --video "episodes/Ep01.mp4" \
    --subs  "subs/Ep01.ja.srt" \
    --words "unknown/Ep01_words.txt" \
    --primary-mdx "dicts/primary" \
    --nhk-old "dicts/NHK_Old" \
    --nhk-new "dicts/NHK_New" \
    --djs "dicts/DJS" \
    --freq "dicts/term_meta_bank_1.json" \
    --outdir "out/Ep01" \
    --csv "out/Ep01/cards.csv" \
    --anki \
    --anki-deck "Japanese::Kimetsu_no_Yaiba" \
    --anki-tags anime kimetsu S01E01

æ³¨æ„äº‹é¡¹:
  - ä½¿ç”¨ --anki å‚æ•°éœ€è¦å…ˆå®‰è£… AnkiConnect æ’ä»¶ (ä»£ç : 2055492159)
  - ç¡®ä¿ Anki æ­£åœ¨è¿è¡Œ
  - ç¬”è®°ç±»å‹ (--anki-model) å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µ:
    word, sentence, sentenceFurigana, sentenceEng, reading, sentenceCard,
    audioCard, notes, picture, wordAudio, sentenceAudio, selectionText,
    definition, glossary, pitchPosition, pitch, frequency, freqSort,
    miscInfo, dictionaryPreference
"""

import argparse
import base64
import csv
import json
import re
import subprocess
import zipfile
from dataclasses import dataclass, asdict
from datetime import timedelta
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any

import pysubs2
import requests
from fugashi import Tagger

try:
    import pandas as pd
except ImportError:
    pd = None

# å¯¼å…¥ mdx_utils æ¨¡å—
from mdx_utils import MeaningsLookup, AudioLookup, get_all_audio_info_from_mdx

# ----------------------- AnkiConnect API -----------------------

class AnkiConnect:
    """AnkiConnect API å°è£…"""
    
    def __init__(self, url: str = "http://localhost:8765"):
        self.url = url
    
    def invoke(self, action: str, **params) -> Any:
        """è°ƒç”¨ AnkiConnect API"""
        payload = {
            "action": action,
            "version": 6,
            "params": params
        }
        
        response = requests.post(self.url, json=payload)
        response.raise_for_status()
        
        result = response.json()
        if len(result) != 2:
            raise Exception('AnkiConnect å“åº”æ ¼å¼é”™è¯¯')
        
        if result['error'] is not None:
            raise Exception(f'AnkiConnect é”™è¯¯: {result["error"]}')
        
        return result['result']
    
    def check_connection(self) -> bool:
        """æ£€æŸ¥ AnkiConnect æ˜¯å¦å¯ç”¨"""
        try:
            version = self.invoke('version')
            print(f"   âœ… AnkiConnect å·²è¿æ¥ (ç‰ˆæœ¬: {version})")
            return True
        except Exception as e:
            print(f"   âŒ AnkiConnect è¿æ¥å¤±è´¥: {e}")
            print("      è¯·ç¡®ä¿ Anki æ­£åœ¨è¿è¡Œä¸”å·²å®‰è£… AnkiConnect æ’ä»¶")
            return False
    
    def create_deck(self, deck_name: str) -> None:
        """åˆ›å»ºç‰Œç»„ (å¦‚æœä¸å­˜åœ¨)"""
        self.invoke('createDeck', deck=deck_name)
    
    def add_note(self, note: Dict[str, Any]) -> int:
        """æ·»åŠ ç¬”è®°,è¿”å›ç¬”è®° ID"""
        return self.invoke('addNote', note=note)
    
    def store_media_file(self, filename: str, data: str) -> str:
        """
        å­˜å‚¨åª’ä½“æ–‡ä»¶åˆ° Anki
        
        Args:
            filename: æ–‡ä»¶å
            data: Base64 ç¼–ç çš„æ•°æ® (ä¸éœ€è¦ data URI å‰ç¼€)
        
        Returns:
            æ–‡ä»¶å (ç”¨äº [sound:...] æˆ– <img> æ ‡ç­¾)
        """
        self.invoke('storeMediaFile', filename=filename, data=data)
        return filename


def format_time_hhmmss(seconds: float) -> str:
    """
    å°†ç§’æ•°æ ¼å¼åŒ–ä¸º (h:)mm:ss
    
    ç¤ºä¾‹:
    - 65.5 â†’ "01:05"
    - 3665.5 â†’ "1:01:05"
    """
    if not seconds or seconds < 0:
        return "00:00"
    
    td = timedelta(seconds=seconds)
    total_seconds = int(td.total_seconds())
    
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    secs = total_seconds % 60
    
    if hours > 0:
        return f"{hours}:{minutes:02d}:{secs:02d}"
    else:
        return f"{minutes:02d}:{secs:02d}"


# ----------------------- Data Types -----------------------

@dataclass
class CardData:
    """Anki å¡ç‰‡æ•°æ®"""
    # åŸºç¡€å­—æ®µ
    word: str                    # ç›®æ ‡å•è¯
    sentence: str                # åŸå¥
    sentence_furigana: str       # å¸¦å‡åçš„åŸå¥
    
    # é‡Šä¹‰
    definition: str              # HTML æ ¼å¼é‡Šä¹‰ (Yomitan)
    
    # éŸ³è°ƒä¿¡æ¯
    reading: str                 # å‡åè¯»éŸ³ (å¸¦ä¸Šåˆ’çº¿æ ‡è®°)
    pitch_position: str          # éŸ³è°ƒä½ç½® [0], [1], [2] ç­‰
    pitch_type: str              # éŸ³è°ƒç±»å‹ (å¹³æ¿å¼/é ­é«˜å‹/ä¸­é«˜å‹/å°¾é«˜å‹)
    pitch_source: str            # éŸ³è°ƒæ¥æºè¯å…¸
    
    # éŸ³é¢‘ (Base64)
    sentence_audio_base64: str   # å¥å­éŸ³é¢‘ Base64
    word_audio_base64: str       # å•è¯éŸ³é¢‘ Base64
    word_audio_source: str       # å•è¯éŸ³é¢‘æ¥æº
    
    # å›¾ç‰‡ (Base64)
    picture_base64: str          # æˆªå›¾ Base64
    
    # é¢‘ç‡
    bccwj_frequency: str         # é¢‘ç‡æ˜¾ç¤ºå€¼
    bccwj_freq_sort: str         # é¢‘ç‡æ’åºå€¼
    
    # åª’ä½“ä¿¡æ¯
    anime_name: str              # åŠ¨æ¼«åç§°
    episode: str                 # é›†æ•° (å¦‚ S01E05)
    
    # æ—¶é—´ä¿¡æ¯
    start_time: float            # å­—å¹•å¼€å§‹æ—¶é—´
    end_time: float              # å­—å¹•ç»“æŸæ—¶é—´
    end_time: float              # å­—å¹•ç»“æŸæ—¶é—´
    
    # é¢å¤–ä¿¡æ¯
    lemma: str                   # è¯å…ƒ
    all_readings: str            # æ‰€æœ‰å€™é€‰è¯»éŸ³ (JSON)


# ----------------------- éŸ³è°ƒç±»å‹è½¬æ¢ -----------------------

def count_kana_length(text: str) -> int:
    """
    è®¡ç®—å‡åçš„æ‹æ•°(ãƒ¢ãƒ¼ãƒ©æ•°)
    
    Args:
        text: å‡åæ–‡æœ¬ (å¹³å‡åæˆ–ç‰‡å‡å)
    
    Returns:
        æ‹æ•°
    
    è§„åˆ™:
    - æ™®é€šå‡å: 1æ‹ (ã‚ã€ã‹ã€ã•...)
    - å°å†™å‡å: 0æ‹,ä¸å‰é¢çš„å‡åç»„æˆ1æ‹ (ã‚ƒã€ã‚…ã€ã‚‡ã€ã‚¡ã€ã‚£ã€ã‚¥...)
    - ä¿ƒéŸ³(ã£/ãƒƒ): 1æ‹
    - é•¿éŸ³(ãƒ¼): 1æ‹
    """
    if not text:
        return 0
    
    # å°å†™å‡å(æ‹—éŸ³ã€å°å†™ç‰‡å‡å)
    small_kana = set('ããƒã…ã‡ã‰ã‚ƒã‚…ã‚‡ã‚ã‚¡ã‚£ã‚¥ã‚§ã‚©ãƒµãƒ¶ãƒ£ãƒ¥ãƒ§ãƒ®')
    
    count = 0
    for char in text:
        # å°å†™å‡åä¸å•ç‹¬è®¡æ•°
        if char in small_kana:
            continue
        # å‡åèŒƒå›´: å¹³å‡å(0x3040-0x309F) æˆ– ç‰‡å‡å(0x30A0-0x30FF)
        code = ord(char)
        if (0x3040 <= code <= 0x309F) or (0x30A0 <= code <= 0x30FF):
            count += 1
    
    return count


def pitch_position_to_type(pitch_position: str, reading: str = "") -> str:
    """
    å°†éŸ³è°ƒä½ç½®è½¬æ¢ä¸ºæ—¥è¯­å£°è°ƒç±»å‹åç§°
    
    Args:
        pitch_position: éŸ³è°ƒä½ç½®,å¦‚ "[0]", "[1]", "[2]" ç­‰
        reading: å‡åè¯»éŸ³ (ç”¨äºè®¡ç®—æ‹æ•°)
    
    Returns:
        å£°è°ƒç±»å‹: å¹³æ¿å¼/é ­é«˜å‹/ä¸­é«˜å‹/å°¾é«˜å‹
    
    è§„åˆ™:
    - [0]: å¹³æ¿å¼ (ã¸ã„ã°ã‚“ã—ã) - ç¬¬ä¸€æ‹ä½,åç»­é«˜ä¸”ä¸é™
    - [1]: é ­é«˜å‹ (ã‚ãŸã¾ã ã‹ãŒãŸ) - ç¬¬ä¸€æ‹é«˜,ç¬¬äºŒæ‹é™
    - [2]~[N-1]: ä¸­é«˜å‹ (ãªã‹ã ã‹ãŒãŸ) - ä¸­é—´æŸå¤„é™è°ƒ
    - [N]: å°¾é«˜å‹ (ãŠã ã‹ãŒãŸ) - æœ€åä¸€æ‹é«˜,åŠ©è¯å¤„é™
    
    ç¤ºä¾‹:
    - ã«ãã‚‹ã¾ [2], æ‹æ•°=4 â†’ ä¸­é«˜å‹ (2 < 4)
    - ãã‚‰ [2], æ‹æ•°=2 â†’ å°¾é«˜å‹ (2 == 2)
    - ãã‚‰ã‚‚ã‚ˆã† [0], æ‹æ•°=5 â†’ å¹³æ¿å¼
    """
    if not pitch_position:
        return ""
    
    # æå–æ•°å­—
    import re
    match = re.search(r'\[(\d+)\]', pitch_position)
    if not match:
        return ""
    
    pos = int(match.group(1))
    
    if pos == 0:
        return "å¹³æ¿å¼"
    elif pos == 1:
        return "é ­é«˜å‹"
    else:
        # éœ€è¦æ ¹æ®å‡åé•¿åº¦åˆ¤æ–­æ˜¯ä¸­é«˜å‹è¿˜æ˜¯å°¾é«˜å‹
        if reading:
            # å»é™¤ HTML æ ‡ç­¾å’Œä¸Šåˆ’çº¿æ ‡è®°
            clean_reading = re.sub(r'<[^>]+>', '', reading)
            mora_count = count_kana_length(clean_reading)
            
            if mora_count > 0 and pos == mora_count:
                return "å°¾é«˜å‹"
        
        return "ä¸­é«˜å‹"


def generate_pitch_html(reading: str, pitch_position: int, pitch_type: str) -> str:
    """
    ç”Ÿæˆå¸¦éŸ³è°ƒæ ‡è®°çš„ HTML (Yomitan é£æ ¼)
    
    Args:
        reading: å‡åè¯»éŸ³ (å¦‚ "ã»ãŸã‚‹", "ã›ã„ã‚Œã„")
        pitch_position: éŸ³è°ƒä½ç½®æ•°å­— (å¦‚ 0, 1, 2, 3)
        pitch_type: éŸ³è°ƒç±»å‹ ("å¹³æ¿å¼", "é ­é«˜å‹", "ä¸­é«˜å‹", "å°¾é«˜å‹")
    
    Returns:
        HTML å­—ç¬¦ä¸²,åŒ…å«éŸ³è°ƒæ ‡è®°å’Œå¯¹åº”é¢œè‰²
    
    é¢œè‰²è§„åˆ™:
    - é ­é«˜å‹ (1å‹): çº¢è‰² (#f54360 æˆ– red)
    - å¹³æ¿å¼ (0å‹): è“è‰² (#39c1ff æˆ– blue)
    - ä¸­é«˜å‹/å°¾é«˜å‹: ç»¿è‰² (#93c47d æˆ– green)
    
    ç¤ºä¾‹:
    - ã»ãŸã‚‹ [1]: çº¢è‰², ç¬¬ä¸€æ‹åä¸‹é™
    - ã›ã„ã‚Œã„ [0]: è“è‰², ç¬¬ä¸€æ‹ä½åç»­é«˜
    - ã«ãã‚‹ã¾ [2]: ç»¿è‰², ç¬¬äºŒæ‹åä¸‹é™
    """
    if not reading:
        return ""
    
    # ç¡®å®šé¢œè‰² (ä½¿ç”¨ match-case è¯­æ³•, Python 3.10+)
    match pitch_type:
        case "é ­é«˜å‹":
            color = "#f54360"  # çº¢è‰² (atamadaka)
        case "å¹³æ¿å¼":
            color = "#39c1ff"  # è“è‰² (heiban)
        case "ä¸­é«˜å‹":
            color = "#fca311"  # æ©™è‰² (nakadaka)
        case "å°¾é«˜å‹":
            color = "#40D4A6"  # é’ç»¿è‰² (odaka)
        case _:
            color = "#afa2ff"  # é»˜è®¤ç»¿è‰²
    
    # æ‹†åˆ†å‡åä¸ºå•ä¸ªå­—ç¬¦(å¤„ç†å°å†™å‡å)
    chars = list(reading)
    
    # ç”Ÿæˆæ¯ä¸ªå‡åçš„ HTML
    spans = []
    for i, char in enumerate(chars):
        mora_index = i + 1  # æ‹æ•°ä» 1 å¼€å§‹
        
        # åŸºç¡€æ ·å¼
        char_html = f'<span style="display:inline;">{char}</span>'
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸Šåˆ’çº¿å’Œä¸‹é™æ ‡è®°
        has_overline = False
        has_drop = False
        
        if pitch_position == 0:
            # å¹³æ¿å¼: ç¬¬ä¸€æ‹æ— çº¿,ç¬¬äºŒæ‹å¼€å§‹æœ‰ä¸Šåˆ’çº¿
            if mora_index > 1:
                has_overline = True
        elif pitch_position == 1:
            # é ­é«˜å‹: ç¬¬ä¸€æ‹æœ‰ä¸Šåˆ’çº¿+ä¸‹é™æ ‡è®°,åç»­æ— çº¿
            if mora_index == 1:
                has_overline = True
                has_drop = True
        else:
            # ä¸­é«˜å‹/å°¾é«˜å‹: ç¬¬äºŒæ‹åˆ°ä¸‹é™ä½ç½®æœ‰ä¸Šåˆ’çº¿,ä¸‹é™ä½ç½®æœ‰æ ‡è®°
            if 2 <= mora_index <= pitch_position:
                has_overline = True
                if mora_index == pitch_position:
                    has_drop = True
        
        # æ„å»ºæ ‡è®° HTML
        mark_style_parts = [
            f'border-color:{color}',
        ]
        
        if has_overline or has_drop:
            mark_style_parts.extend([
                'display:block',
                'user-select:none',
                'pointer-events:none',
                'position:absolute',
                'top:0.1em',
                'left:0',
                'right:0',
                'height:0',
                'border-top-width:0.1em',
                'border-top-style:solid',
            ])
        
        if has_drop:
            # ä¸‹é™æ ‡è®°: å³ä¾§ç«–çº¿
            mark_style_parts.extend([
                'right:-0.1em',
                'height:0.4em',
                'border-right-width:0.1em',
                'border-right-style:solid',
            ])
        
        mark_html = f'<span style="{";".join(mark_style_parts)};"></span>'
        
        # åŒ…è£…å®¹å™¨
        container_style_parts = [
            'display:inline-block',
            'position:relative',
        ]
        
        if has_drop:
            # ä¸‹é™ä½ç½®éœ€è¦å³è¾¹è·å’Œå†…è¾¹è·
            container_style_parts.extend([
                'padding-right:0.1em',
                'margin-right:0.1em',
            ])
        
        container_html = f'<span style="{";".join(container_style_parts)};">{char_html}{mark_html}</span>'
        spans.append(container_html)
    
    # ç»„åˆæ‰€æœ‰å‡å
    full_html = '<span style="display:inline;">' + ''.join(spans) + '</span>'
    
    return full_html


def file_to_base64(file_path: Path) -> str:
    """
    å°†æ–‡ä»¶è½¬æ¢ä¸º Base64 ç¼–ç å­—ç¬¦ä¸²
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
    
    Returns:
        Base64 ç¼–ç çš„å­—ç¬¦ä¸²,å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    import base64
    
    if not file_path or not file_path.exists():
        return ""
    
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
            b64 = base64.b64encode(data).decode('utf-8')
            
            # æ·»åŠ  data URI scheme
            ext = file_path.suffix.lower()
            if ext in ['.png', '.jpg', '.jpeg', '.gif', '.webp']:
                mime_type = f"image/{ext[1:]}"
                if ext == '.jpg':
                    mime_type = "image/jpeg"
                return f"data:{mime_type};base64,{b64}"
            elif ext in ['.mp3', '.m4a', '.ogg', '.wav']:
                mime_type = f"audio/{ext[1:]}"
                if ext == '.m4a':
                    mime_type = "audio/mp4"
                return f"data:{mime_type};base64,{b64}"
            else:
                return f"data:application/octet-stream;base64,{b64}"
    except Exception as e:
        print(f"   âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return ""


# ----------------------- FFmpeg è¾…åŠ©å‡½æ•° -----------------------

def ms_to_s(ms: int) -> float:
    """æ¯«ç§’è½¬ç§’"""
    return ms / 1000.0


def ensure_dir(p: Path) -> None:
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    p.mkdir(parents=True, exist_ok=True)


def run_ffmpeg(cmd: List[str]) -> None:
    """æ‰§è¡Œ FFmpeg å‘½ä»¤"""
    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if proc.returncode != 0:
        raise RuntimeError(
            f"FFmpeg failed: {' '.join(cmd)}\n{proc.stderr.decode('utf-8', 'ignore')}"
        )


def screenshot(video: Path, t: float, out_jpg: Path, vf: Optional[str] = None) -> None:
    """æˆªå–è§†é¢‘å¸§å¹¶ä¿å­˜ä¸º JPG (95% è´¨é‡)"""
    cmd = ["ffmpeg", "-y", "-ss", f"{t:.3f}", "-i", str(video)]
    if vf:
        cmd += ["-vf", vf]
    # ä½¿ç”¨ JPEG ç¼–ç å™¨,qscale:v 2 çº¦ç­‰äº 95% è´¨é‡
    # qscale:v èŒƒå›´: 2-31, 2=æœ€é«˜è´¨é‡, 31=æœ€ä½è´¨é‡
    cmd += ["-vframes", "1", "-c:v", "mjpeg", "-q:v", "2", str(out_jpg)]
    run_ffmpeg(cmd)


def cut_audio(video: Path, start: float, end: float, out_audio: Path) -> None:
    """è£å‰ªéŸ³é¢‘ç‰‡æ®µ"""
    dur = max(0.01, end - start)
    cmd = [
        "ffmpeg", "-y", "-ss", f"{start:.3f}", "-t", f"{dur:.3f}",
        "-i", str(video), "-vn", "-ac", "2", "-ar", "48000",
        "-c:a", "aac", "-b:a", "192k", str(out_audio)
    ]
    run_ffmpeg(cmd)


# ----------------------- å­—å¹•å’Œæ–‡æœ¬å¤„ç† -----------------------

def katakana_to_hiragana(text: str) -> str:
    """å°†ç‰‡å‡åè½¬æ¢ä¸ºå¹³å‡å"""
    if not text:
        return ""
    result = []
    for char in text:
        code = ord(char)
        # ç‰‡å‡åèŒƒå›´: 0x30A0-0x30FF
        # å¹³å‡åèŒƒå›´: 0x3040-0x309F
        # åç§»é‡: 0x60
        if 0x30A1 <= code <= 0x30F6:  # ã‚«-ãƒ¶
            result.append(chr(code - 0x60))
        else:
            result.append(char)
    return ''.join(result)


def is_all_katakana(text: str) -> bool:
    """æ£€æµ‹æ–‡æœ¬æ˜¯å¦å…¨æ˜¯ç‰‡å‡å(å…è®¸é•¿éŸ³ç¬¦ã€ä¿ƒéŸ³ç­‰)
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        
    Returns:
        True å¦‚æœå…¨æ˜¯ç‰‡å‡åå­—ç¬¦
        
    Example:
        >>> is_all_katakana("ã‚³ãƒ¼ãƒ’ãƒ¼")
        True
        >>> is_all_katakana("ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’é£²ã‚€")
        False
    """
    if not text:
        return False
    # ç‰‡å‡åèŒƒå›´: ã‚¡-ãƒ¶ (0x30A1-0x30F6) + é•¿éŸ³ç¬¦ ãƒ¼ (0x30FC) + ä¸­ç‚¹ ãƒ» (0x30FB)
    for char in text:
        code = ord(char)
        if not (0x30A1 <= code <= 0x30F6 or code == 0x30FC or code == 0x30FB):
            return False
    return True


def expand_long_vowel(text: str) -> str:
    """å±•å¼€é•¿éŸ³ç¬¦(ãƒ¼)ä¸ºå®Œæ•´å‡å
    
    Args:
        text: å¹³å‡åæ–‡æœ¬ (å¯èƒ½åŒ…å«é•¿éŸ³ç¬¦)
        
    Returns:
        å±•å¼€åçš„å¹³å‡å (æ— é•¿éŸ³ç¬¦)
        
    è§„åˆ™:
    - ã‚ãƒ¼ â†’ ã‚a
    - ã„ãƒ¼ â†’ ã„i
    - ã†ãƒ¼ â†’ ã†u
    - ãˆãƒ¼ â†’ ãˆãˆ/ãˆã„ (æ ¹æ®å‰å­—åˆ¤æ–­)
    - ãŠãƒ¼ â†’ ã†o (ãŠæ®µé•¿éŸ³é€šå¸¸ç”¨ ã†)
    
    Example:
        >>> expand_long_vowel("ã“ãƒ¼ã²ãƒ¼")
        "ã“ã†ã²ã„"
        >>> expand_long_vowel("ã›ã‚“ã›ãƒ¼")
        "ã›ã‚“ã›ã„"
    """
    if not text or 'ãƒ¼' not in text:
        return text
    
    # æ¯éŸ³æ˜ å°„ (å¹³å‡åè¡Œçš„ä»£è¡¨å­—ç¬¦)
    vowel_map = {
        'ã‚': 'ã‚', 'ã‹': 'ã‚', 'ã•': 'ã‚', 'ãŸ': 'ã‚', 'ãª': 'ã‚', 
        'ã¯': 'ã‚', 'ã¾': 'ã‚', 'ã‚„': 'ã‚', 'ã‚‰': 'ã‚', 'ã‚': 'ã‚', 'ãŒ': 'ã‚', 'ã–': 'ã‚', 'ã ': 'ã‚', 'ã°': 'ã‚', 'ã±': 'ã‚',
        'ã„': 'ã„', 'ã': 'ã„', 'ã—': 'ã„', 'ã¡': 'ã„', 'ã«': 'ã„', 
        'ã²': 'ã„', 'ã¿': 'ã„', 'ã‚Š': 'ã„', 'ã‚': 'ã„', 'ã': 'ã„', 'ã˜': 'ã„', 'ã¢': 'ã„', 'ã³': 'ã„', 'ã´': 'ã„',
        'ã†': 'ã†', 'ã': 'ã†', 'ã™': 'ã†', 'ã¤': 'ã†', 'ã¬': 'ã†', 
        'ãµ': 'ã†', 'ã‚€': 'ã†', 'ã‚†': 'ã†', 'ã‚‹': 'ã†', 'ã': 'ã†', 'ãš': 'ã†', 'ã¥': 'ã†', 'ã¶': 'ã†', 'ã·': 'ã†',
        'ãˆ': 'ã„', 'ã‘': 'ã„', 'ã›': 'ã„', 'ã¦': 'ã„', 'ã­': 'ã„', 
        'ã¸': 'ã„', 'ã‚': 'ã„', 'ã‚Œ': 'ã„', 'ã‚‘': 'ã„', 'ã’': 'ã„', 'ãœ': 'ã„', 'ã§': 'ã„', 'ã¹': 'ã„', 'ãº': 'ã„',
        'ãŠ': 'ã†', 'ã“': 'ã†', 'ã': 'ã†', 'ã¨': 'ã†', 'ã®': 'ã†', 
        'ã»': 'ã†', 'ã‚‚': 'ã†', 'ã‚ˆ': 'ã†', 'ã‚': 'ã†', 'ã‚’': 'ã†', 'ã”': 'ã†', 'ã': 'ã†', 'ã©': 'ã†', 'ã¼': 'ã†', 'ã½': 'ã†',
        'ã‚“': 'ã‚“',
    }
    
    result = []
    for i, char in enumerate(text):
        if char == 'ãƒ¼' and i > 0:
            # é•¿éŸ³ç¬¦: é‡å¤å‰ä¸€ä¸ªå­—ç¬¦çš„æ¯éŸ³
            prev_char = result[-1] if result else 'ã‚'
            vowel = vowel_map.get(prev_char, 'ã†')  # é»˜è®¤ u éŸ³
            result.append(vowel)
        else:
            result.append(char)
    
    return ''.join(result)


def normalize_sub_text(s: str) -> str:
    """æ ‡å‡†åŒ–å­—å¹•æ–‡æœ¬,å»é™¤æ ·å¼æ ‡ç­¾"""
    if not s:
        return ""
    s = re.sub(r"\{[^}]*\}", "", s)  # ASS æ ‡ç­¾
    s = re.sub(r"<[^>]+>", "", s)    # HTML æ ‡ç­¾
    s = s.replace("\\N", "\n")       # ASS æ¢è¡Œæ ‡è®° -> çœŸå®æ¢è¡Œ
    s = s.replace("\u3000", " ")     # å…¨è§’ç©ºæ ¼
    s = re.sub(r"\s+", " ", s).strip()
    return s


def extract_episode_info(video_path: Path, words_path: Path) -> tuple[str, str]:
    """ä»æ–‡ä»¶åæå–åŠ¨æ¼«åå’Œé›†æ•°ä¿¡æ¯
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        words_path: å•è¯æ–‡ä»¶è·¯å¾„
        
    Returns:
        (anime_name, episode): åŠ¨æ¼«åå’Œé›†æ•° (å¦‚ "é¬¼ç­ä¹‹åˆƒ", "S01E05")
    """
    # ä»è§†é¢‘æ–‡ä»¶åæå–åŠ¨æ¼«å (å»æ‰æ‰©å±•åå’Œé›†æ•°ä¿¡æ¯)
    video_stem = video_path.stem  # ä¾‹å¦‚: "[BeanSub&FZSD&VCB-Studio] Kimetsu no Yaiba [01][Ma10p 1080p][x265 flac aac]"
    
    # ä»å•è¯æ–‡ä»¶åæå–é›†æ•° (åŒ¹é… S_E æˆ– SE æ ¼å¼)
    words_stem = words_path.stem  # ä¾‹å¦‚: "S1_E2_words" æˆ– "S01E05_words"
    
    # å°è¯•å¤šç§æ ¼å¼åŒ¹é…
    episode_code = None
    
    # 1. åŒ¹é… Sx_Ex æ ¼å¼ (S1_E2, S01_E05 ç­‰ï¼Œç”¨ä¸‹åˆ’çº¿åˆ†éš”)
    episode_match = re.search(r'S(\d+)_E(\d+)', words_stem, re.IGNORECASE)
    if episode_match:
        season = episode_match.group(1)
        episode = episode_match.group(2)
        episode_code = f"S{season.zfill(2)}E{episode.zfill(2)}"
    
    # 2. åŒ¹é… SxEx æ ¼å¼ (S1E2, S01E05 ç­‰ï¼Œæ— ä¸‹åˆ’çº¿)
    if not episode_code:
        episode_match = re.search(r'S(\d+)E(\d+)', words_stem, re.IGNORECASE)
        if episode_match:
            season = episode_match.group(1)
            episode = episode_match.group(2)
            episode_code = f"S{season.zfill(2)}E{episode.zfill(2)}"
    
    # 3. å¦‚æœæ²¡æ‰¾åˆ°,å°è¯•åŒ¹é… Ep01 æ ¼å¼
    if not episode_code:
        ep_match = re.search(r'Ep(\d+)', words_stem, re.IGNORECASE)
        if ep_match:
            episode_code = f"S01E{ep_match.group(1).zfill(2)}"
    
    # 4. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°,å°è¯•ä»è§†é¢‘æ–‡ä»¶åæå–
    if not episode_code:
        video_episode_match = re.search(r'S(\d+)E(\d+)', video_stem, re.IGNORECASE)
        if video_episode_match:
            season = video_episode_match.group(1)
            episode = video_episode_match.group(2)
            episode_code = f"S{season.zfill(2)}E{episode.zfill(2)}"
        else:
            # å°è¯•åŒ¹é…æ–¹æ‹¬å·ä¸­çš„æ•°å­— [01]
            bracket_match = re.search(r'\[(\d{1,2})\]', video_stem)
            if bracket_match:
                episode_code = f"S01E{bracket_match.group(1).zfill(2)}"
            else:
                episode_code = "S01E01"  # é»˜è®¤å€¼
    
    # ä»è§†é¢‘æ–‡ä»¶åæå–åŠ¨æ¼«å
    # 1. ç§»é™¤æ‰€æœ‰æ–¹æ‹¬å·åŠå…¶å†…å®¹ (å¦‚ [BeanSub&FZSD&VCB-Studio], [01], [Ma10p 1080p] ç­‰)
    anime_name = re.sub(r'\[[^\]]*\]', '', video_stem)
    
    # 2. ç§»é™¤é›†æ•°éƒ¨åˆ† (S01E05 æˆ– Ep01 æ ¼å¼)
    anime_name = re.sub(r'[_\s]*S\d+E\d+.*', '', anime_name, flags=re.IGNORECASE)
    anime_name = re.sub(r'[_\s]*Ep\d+.*', '', anime_name, flags=re.IGNORECASE)
    
    # 3. ä¸‹åˆ’çº¿è½¬ç©ºæ ¼,æ¸…ç†å¤šä½™ç©ºç™½
    anime_name = anime_name.replace('_', ' ').strip()
    anime_name = re.sub(r'\s+', ' ', anime_name)  # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
    
    if not anime_name:
        anime_name = video_stem  # å¦‚æœæå–å¤±è´¥,ä½¿ç”¨å®Œæ•´æ–‡ä»¶å
    
    return anime_name, episode_code


def load_words(path: Path) -> List[Tuple[str, Optional[str], Optional[str]]]:
    """ä»æ–‡ä»¶åŠ è½½ç›®æ ‡å•è¯åˆ—è¡¨
    
    æ”¯æŒæ ¼å¼:
    - æ™®é€šå•è¯: ç²¾éœŠ
    - å•åœ†æ‹¬å·æŒ‡å®šè¯»éŸ³: ç²¾éœŠ(ã›ã„ã‚Œã„)
    - å•æ–¹æ‹¬å·æŒ‡å®šæŸ¥è¯å½¢æ€: é£Ÿã¹ãŸ[é£Ÿã¹ã‚‹] æˆ– ç‹‚ã„å’²ã[ç‹‚ã„å’²ã]
    - åŒæ—¶æŒ‡å®šè¯»éŸ³å’ŒæŸ¥è¯å½¢æ€: é£Ÿã¹ãŸ(ãŸã¹ãŸ)[é£Ÿã¹ã‚‹]
    
    è¯­æ³•è§„åˆ™:
    - () åœ†æ‹¬å· = å¼ºåˆ¶è¯»éŸ³
    - [] æ–¹æ‹¬å· = å¼ºåˆ¶æŸ¥è¯å½¢æ€
    
    Returns:
        List[Tuple[str, Optional[str], Optional[str]]]: [(å•è¯, è¯»éŸ³æˆ–None, æŸ¥è¯å½¢æ€æˆ–None), ...]
    """
    txt = path.read_text(encoding='utf-8')
    words_with_reading = []
    
    for w in re.split(r"[\n,\t]", txt):
        w = w.strip()
        if not w:
            continue
        
        lookup_form = None
        reading = None
        original_w = w
        
        # æ£€æŸ¥æ–¹æ‹¬å·(æŸ¥è¯å½¢æ€): é£Ÿã¹ãŸ[é£Ÿã¹ã‚‹]
        dict_match = re.search(r'\[([^\]]+)\]', w)
        if dict_match:
            lookup_form = dict_match.group(1).strip()
            # ç§»é™¤æ–¹æ‹¬å·éƒ¨åˆ†
            w = w.replace(dict_match.group(0), '')
        
        # æ£€æŸ¥åœ†æ‹¬å·(è¯»éŸ³): ç²¾éœŠ(ã›ã„ã‚Œã„)
        reading_match = re.search(r'\(([^\)]+)\)', w)
        if reading_match:
            reading = reading_match.group(1).strip()
            # ç§»é™¤åœ†æ‹¬å·éƒ¨åˆ†
            w = w.replace(reading_match.group(0), '')
        
        # å‰©ä¸‹çš„å°±æ˜¯å•è¯æœ¬èº«
        word = w.strip()
        
        words_with_reading.append((word, reading, lookup_form))
    
    return words_with_reading


def tokens_furigana(text: str, tagger: Tagger) -> str:
    """ä¸ºæ–‡æœ¬æ·»åŠ å‡åæ³¨éŸ³(å¹³å‡å)
    
    æ ¼å¼è§„åˆ™:
    - æœ‰æ±‰å­—çš„è¯æ·»åŠ å‡åæ ‡æ³¨: é–“é•[ã¾ã¡ãŒ]
    - é€ã‚Šä»®åä¸åŒ…å«åœ¨æ–¹æ‹¬å·å†…: é–“é•[ã¾ã¡ãŒ]ã„ (ä¸æ˜¯ é–“é•ã„[ã¾ã¡ãŒã„])
    - ä»ç¬¬äºŒä¸ªtokenå¼€å§‹,æ¯ä¸ªtokenå‰åŠ ç©ºæ ¼: é–“é•[ã¾ã¡ãŒ]ã„ ä»Š[ã„ã¾]ã¯
    
    ç¤ºä¾‹:
        é–“é•[ã¾ã¡ãŒ]ã„ ä»Š[ã„ã¾]ã¯ é‡åº¦[ã˜ã‚…ã†ã©]ã® é£¢é¤“[ããŒ]çŠ¶æ…‹[ã˜ã‚‡ã†ãŸã„]
    """
    if not text:
        return ""
    out = []
    for t in tagger(text):
        surf = t.surface
        # è·å–è¯»éŸ³(ç‰‡å‡å)
        yomi = None
        if hasattr(t.feature, 'kana'):
            yomi = t.feature.kana
        elif hasattr(t, 'feature') and len(t.feature) > 7:
            yomi = t.feature[7]
        
        # è½¬æ¢ä¸ºå¹³å‡å
        if yomi:
            yomi = katakana_to_hiragana(yomi)
        
        # å¦‚æœæœ‰æ±‰å­—ä¸”è¯»éŸ³ä¸åŒ,æ·»åŠ å‡å
        if yomi and yomi != surf and re.search(r"[ä¸€-é¾¯ã€…ã€†ãƒµãƒ¶]", surf):
            # åˆ†ç¦»æ±‰å­—éƒ¨åˆ†å’Œé€ã‚Šä»®å
            # æ‰¾åˆ°æœ€åä¸€ä¸ªæ±‰å­—çš„ä½ç½®
            kanji_end = 0
            for i, char in enumerate(surf):
                if re.match(r"[ä¸€-é¾¯ã€…ã€†ãƒµãƒ¶]", char):
                    kanji_end = i + 1
            
            if kanji_end < len(surf):
                # æœ‰é€ã‚Šä»®å: é–“é•ã„ -> é–“é•[ã¾ã¡ãŒ]ã„
                kanji_part = surf[:kanji_end]
                okurigana = surf[kanji_end:]
                
                # å‡åä¹Ÿè¦å¯¹åº”æˆªå–(å»æ‰é€ã‚Šä»®åå¯¹åº”çš„éƒ¨åˆ†)
                # ç®€å•å¤„ç†: å‡è®¾é€ã‚Šä»®åçš„å‡åå°±æ˜¯å®ƒæœ¬èº«
                yomi_kanji = yomi
                if okurigana and yomi.endswith(okurigana):
                    yomi_kanji = yomi[:-len(okurigana)]
                
                out.append(f"{kanji_part}[{yomi_kanji}]{okurigana}")
            else:
                # å…¨æ˜¯æ±‰å­—: é–“é• -> é–“é•[ã¾ã¡ãŒã„]
                out.append(f"{surf}[{yomi}]")
        else:
            out.append(surf)
    
    # ä»ç¬¬äºŒä¸ªtokenå¼€å§‹,æ¯ä¸ªå‰é¢åŠ ç©ºæ ¼
    return ' '.join(out)


def lemmatize(text: str, tagger: Tagger) -> List[str]:
    """è·å–æ–‡æœ¬ä¸­æ‰€æœ‰è¯çš„è¯å…ƒå½¢å¼"""
    if not text:
        return []
    lemmas = []
    for t in tagger(text):
        # å°è¯•å¤šç§æ–¹å¼è·å–è¯å…ƒ
        lemma = None
        if hasattr(t.feature, 'lemma'):
            lemma = t.feature.lemma
        elif hasattr(t, 'feature') and len(t.feature) > 6:
            lemma = t.feature[6]  # IPADic æ ¼å¼
        
        lemmas.append(lemma or t.surface)
    return lemmas


# ----------------------- é¢‘ç‡ç´¢å¼• -----------------------

class FrequencyIndex:
    """é¢‘ç‡æ•°æ®ç´¢å¼• (æ”¯æŒ CSV/TSV/JSON æ ¼å¼)"""
    
    def __init__(self, path: Optional[Path] = None):
        self.idx: Dict[str, Tuple[str, float]] = {}
        if not path:
            return
        
        try:
            if path.suffix.lower() == '.json':
                self._load_from_json(path)
            elif path.suffix.lower() == '.zip':
                self._load_from_zip(path)
            else:
                self._load_from_file(path)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½é¢‘ç‡æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _load_from_json(self, path: Path):
        """ä» Yomichan term_meta_bank JSON æ–‡ä»¶åŠ è½½
        
        æ ¼å¼ç¤ºä¾‹:
        ["å…·ä½“çš„æ¤œè¨", "freq", {"reading": "ããŸã„ã¦ãã‘ã‚“ã¨ã†", "frequency": {"value": 108366, "displayValue": "10ä¸‡"}}]
        æˆ–ç®€å•æ ¼å¼:
        ["å˜èª", "freq", æ•°å€¼]
        ["å˜èª", "freq", {"value": æ•°å€¼, "displayValue": "æ˜¾ç¤º"}]
        """
        import json
        
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            print(f"   âš ï¸  JSON æ ¼å¼ä¸æ­£ç¡®,æœŸæœ›åˆ—è¡¨")
            return
        
        loaded = 0
        for entry in data:
            if not isinstance(entry, list) or len(entry) < 3:
                continue
            
            term = entry[0]  # å•è¯
            meta_type = entry[1]  # é€šå¸¸æ˜¯ "freq"
            meta_value = entry[2]  # é¢‘ç‡å€¼æˆ–å¯¹è±¡
            
            # åªå¤„ç†é¢‘ç‡æ•°æ®
            if meta_type != "freq":
                continue
            
            # æå–æ•°å€¼
            rank = None
            display = None
            
            if isinstance(meta_value, (int, float)):
                # ç®€å•æ•°å€¼æ ¼å¼
                rank = float(meta_value)
                display = f"{int(rank)}"
            elif isinstance(meta_value, dict):
                # å¯¹è±¡æ ¼å¼ - æ£€æŸ¥æ˜¯å¦æœ‰åµŒå¥—çš„ frequency å­—æ®µ
                if 'frequency' in meta_value and isinstance(meta_value['frequency'], dict):
                    # åµŒå¥—æ ¼å¼: {"reading": "...", "frequency": {"value": ..., "displayValue": "..."}}
                    freq_obj = meta_value['frequency']
                    rank = float(freq_obj.get('value', 0))
                    display = f"{freq_obj.get('displayValue', str(int(rank)))}"
                elif 'value' in meta_value:
                    # ç›´æ¥æ ¼å¼: {"value": ..., "displayValue": "..."}
                    rank = float(meta_value.get('value', 0))
                    display = f"{meta_value.get('displayValue', str(int(rank)))}"
                else:
                    continue
            else:
                continue
            
            if rank is not None and display:
                # å­˜å‚¨: term -> (display_string, numeric_rank)
                self.idx.setdefault(term, (display, rank))
                loaded += 1
        
        if loaded > 0:
            print(f"   âœ… åŠ è½½é¢‘ç‡æ•°æ®: {loaded} æ¡")
        else:
            print(f"   âš ï¸  æœªæ‰¾åˆ°ä»»ä½•é¢‘ç‡æ•°æ®")
    
    def _load_from_zip(self, path: Path):
        """ä» ZIP æ–‡ä»¶åŠ è½½"""
        if pd is None:
            return
        
        with zipfile.ZipFile(path) as z:
            cand = [n for n in z.namelist() 
                   if n.lower().endswith(('.csv', '.tsv'))]
            if not cand:
                return
            
            with z.open(cand[0]) as f:
                sep = ',' if cand[0].lower().endswith('.csv') else '\t'
                df = pd.read_csv(f, sep=sep)
                self._load_dataframe(df)
    
    def _load_from_file(self, path: Path):
        """ä» CSV/TSV æ–‡ä»¶åŠ è½½"""
        if pd is None:
            return
        
        sep = ',' if path.suffix.lower() == '.csv' else '\t'
        df = pd.read_csv(path, sep=sep)
        self._load_dataframe(df)
    
    def _load_dataframe(self, df: pd.DataFrame):
        """ä» DataFrame åŠ è½½æ•°æ®"""
        cols = {c.lower(): c for c in df.columns}
        
        # æŸ¥æ‰¾è¯æ¡åˆ—
        term_c = next((cols[k] for k in ['term', 'lemma', 'word', 'è¡¨è¨˜', 'èªå½™', 'èª'] 
                      if k in cols), None)
        # æŸ¥æ‰¾é¢‘ç‡åˆ—
        rank_c = next((cols[k] for k in ['rank', 'freq_rank', 'harmonic_rank', 
                                         'frequency', 'é »åº¦', 'å‡ºç¾åº¦'] 
                      if k in cols), None)
        
        if term_c is None or rank_c is None:
            return
        
        for _, row in df[[term_c, rank_c]].dropna().iterrows():
            term = str(row[term_c])
            try:
                rank = float(row[rank_c])
                self.idx.setdefault(term, (str(rank), rank))
            except (ValueError, TypeError):
                continue
    
    def lookup(self, key: str) -> Tuple[Optional[str], Optional[float]]:
        """æŸ¥è¯¢è¯çš„é¢‘ç‡"""
        return self.idx.get(key, (None, None))


# ----------------------- æ ¸å¿ƒå¤„ç†é€»è¾‘ -----------------------

def find_hits(
    words: List[Tuple[str, Optional[str]]],  # ä¿®æ”¹: ç°åœ¨åŒ…å«å¯é€‰è¯»éŸ³
    subs: pysubs2.SSAFile,
    tagger: Tagger,
    video: Path,
    outdir: Path,
    meanings_lookup: MeaningsLookup,
    audio_lookup: AudioLookup,
    freq_index: FrequencyIndex,
    anime_name: str = "",
    episode: str = "",
    dicts_dir: Optional[Path] = None,
    pad: float = 0.0,
    vf: Optional[str] = None,
    verbose: bool = True
) -> List[CardData]:
    """
    æŸ¥æ‰¾å­—å¹•ä¸­çš„ç›®æ ‡å•è¯å¹¶ç”Ÿæˆå¡ç‰‡æ•°æ®
    
    Args:
        words: ç›®æ ‡å•è¯åˆ—è¡¨ (å•è¯, å¯é€‰è¯»éŸ³)
        subs: å­—å¹•æ–‡ä»¶
        tagger: Fugashi åˆ†è¯å™¨
        video: è§†é¢‘æ–‡ä»¶è·¯å¾„
        outdir: è¾“å‡ºç›®å½•
        meanings_lookup: é‡Šä¹‰æŸ¥è¯¢å¯¹è±¡
        audio_lookup: éŸ³é¢‘æŸ¥è¯¢å¯¹è±¡
        freq_index: é¢‘ç‡ç´¢å¼•
        anime_name: åŠ¨æ¼«åç§°
        episode: é›†æ•°ä¿¡æ¯ (å¦‚ S01E05)
        dicts_dir: è¯å…¸ç›®å½•è·¯å¾„ (ç”¨äº DJS_N å¤‡é€‰éŸ³é¢‘)
        pad: éŸ³é¢‘è£å‰ªå‰åå¡«å……æ—¶é—´(ç§’)
        vf: FFmpeg è§†é¢‘æ»¤é•œ
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        å¡ç‰‡æ•°æ®åˆ—è¡¨
    """
    ensure_dir(outdir)
    cards: List[CardData] = []
    
    # åˆ›å»ºå•è¯é›†åˆå’Œæ˜ å°„
    word_to_reading = {word: reading for word, reading, _ in words}
    word_to_lookup_form = {word: lookup_form for word, _, lookup_form in words}
    wset = set(word_to_reading.keys())
    
    print(f"\nğŸ” å¼€å§‹å¤„ç†å­—å¹•...")
    print(f"   ç›®æ ‡å•è¯: {len(words)} ä¸ª")
    print(f"   å­—å¹•è¡Œæ•°: {len(subs)} è¡Œ\n")
    
    for idx, line in enumerate(subs, 1):
        # æ ‡å‡†åŒ–å­—å¹•æ–‡æœ¬
        sent = normalize_sub_text(line.text)
        if not sent:
            continue
        
        # åˆ†è¯å¹¶è·å–è¯å…ƒ
        lemmas = lemmatize(sent, tagger)
        tokens_set = set(lemmas)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡å•è¯ (ä½¿ç”¨ä¸¤ç§æ–¹å¼: è¯å…ƒåŒ¹é… + å­—ç¬¦ä¸²åŒ¹é…)
        # 1. è¯å…ƒåŒ¹é…: æ£€æŸ¥åˆ†è¯åçš„è¯å…ƒ
        matched_by_lemma = wset.intersection(tokens_set)
        
        # 2. å­—ç¬¦ä¸²åŒ¹é…: ç›´æ¥åœ¨å¥å­ä¸­æŸ¥æ‰¾ (å¤„ç†åˆ†è¯å¤±è´¥çš„æƒ…å†µ)
        matched_by_string = {word for word, _, _ in words if word in sent}
        
        # åˆå¹¶ä¸¤ç§åŒ¹é…ç»“æœ
        matched = matched_by_lemma | matched_by_string
        
        if not matched:
            continue
        
        if verbose:
            print(f"[{idx}/{len(subs)}] æ‰¾åˆ°åŒ¹é…: {', '.join(matched)}")
            print(f"         åŸå¥: {sent[:50]}...")
        
        # ç”Ÿæˆå¸¦å‡åçš„å¥å­
        furig = tokens_furigana(sent, tagger)
        
        # è®¡ç®—æ—¶é—´èŒƒå›´
        start = max(0.0, ms_to_s(line.start) - pad)
        end = ms_to_s(line.end) + pad
        mid = (start + end) / 2
        
        # ç”Ÿæˆæ–‡ä»¶å (ä½¿ç”¨ JPG æ ¼å¼)
        base = f"{video.stem}_{int(line.start)}_{int(line.end)}"
        img_path = outdir / f"{base}.jpg"
        aud_path = outdir / f"{base}.m4a"
        
        # æˆªå›¾å’Œè£å‰ªéŸ³é¢‘
        try:
            screenshot(video, mid, img_path, vf)
            cut_audio(video, start, end, aud_path)
        except Exception as e:
            print(f"   âš ï¸  åª’ä½“å¤„ç†å¤±è´¥: {e}")
            continue
        
        # ä¸ºæ¯ä¸ªåŒ¹é…çš„å•è¯åˆ›å»ºå¡ç‰‡
        for word in matched:
            if verbose:
                print(f"   ğŸ“ æŸ¥è¯¢å•è¯: {word}")
            
            # è®°å½•åŸå§‹å•è¯(å¯èƒ½æ˜¯ç‰‡å‡å)
            original_word = word
            is_original_katakana = is_all_katakana(word)
            
            # è·å–å•è¯çš„è¯å…ƒå½¢å¼ (åŸå½¢) - ç”¨äºè¯å…¸æŸ¥è¯¢
            # ä¾‹å¦‚: é£Ÿã¹ãŸ â†’ é£Ÿã¹ã‚‹, è¦‹ã¦ã„ã‚‹ â†’ è¦‹ã‚‹
            # æ³¨æ„: Fugashi å°†ä¼šæŠŠè¯æ‹†æˆå¤šä¸ª token, æ¯ä¸ª token æœ‰è‡ªå·±çš„ lemma
            # å¯¹äºå¤åˆè¯(ä¾‹å¦‚ ç©ºæ¨¡æ§˜)æˆ‘ä»¬éœ€è¦æŠŠæ‰€æœ‰ token çš„ lemma æ‹¼æ¥èµ·æ¥, è€Œä¸æ˜¯åªå–ç¬¬ä¸€ä¸ª
            word_lemma_parts: List[str] = []
            for t in tagger(word):
                part_lemma = None
                if hasattr(t.feature, 'lemma') and t.feature.lemma:
                    part_lemma = t.feature.lemma
                elif hasattr(t, 'feature') and len(t.feature) > 6 and t.feature[6]:
                    part_lemma = t.feature[6]  # IPADic æ ¼å¼
                else:
                    part_lemma = t.surface
                # ä¿è¯ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                if part_lemma:
                    word_lemma_parts.append(part_lemma)

            # å°†å„ token çš„ lemma æ‹¼æ¥å›å®Œæ•´è¯å…ƒ
            word_lemma = ''.join(word_lemma_parts) if word_lemma_parts else word
            
            # å¦‚æœè¯å…ƒå’ŒåŸè¯ä¸åŒ,æ˜¾ç¤ºæç¤º
            if word_lemma != word and verbose:
                print(f"      ğŸ“– è¯å…ƒå½¢å¼: {word} â†’ {word_lemma}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æŒ‡å®šçš„æŸ¥è¯å½¢æ€ (æ–¹æ‹¬å·è¯­æ³•)
            user_lookup_form = word_to_lookup_form.get(word)
            if user_lookup_form:
                if verbose:
                    print(f"      ğŸ¯ ç”¨æˆ·æŒ‡å®šæŸ¥è¯å½¢æ€: {user_lookup_form}")
                # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„å½¢æ€ä½œä¸ºé¦–é€‰æŸ¥è¯¢è¯
                word_lemma = user_lookup_form
            
            # å‡†å¤‡æŸ¥è¯¢å€™é€‰è¯åˆ—è¡¨ (ç”¨äºå›é€€æŸ¥è¯¢)
            # å¯¹äºç‰‡å‡åè¯æ±‡: åŸç‰‡å‡å â†’ è¯å…ƒ(å¹³å‡å) â†’ ãâ†’ãå˜ä½“
            # å¯¹äºå…¶ä»–è¯æ±‡: è¯å…ƒ â†’ åŸè¯ â†’ ãâ†’ãå˜ä½“
            query_candidates = []
            
            if is_original_katakana and not user_lookup_form:
                # ç‰‡å‡åè¯æ±‡ä¼˜å…ˆç”¨åŸç‰‡å‡åæŸ¥è¯¢
                query_candidates.append(original_word)
                if word_lemma != original_word:
                    query_candidates.append(word_lemma)  # å¹³å‡åä½œä¸ºå¤‡é€‰
            else:
                # å…¶ä»–æƒ…å†µæŒ‰æ­£å¸¸é¡ºåº
                query_candidates.append(word_lemma)
                if word_lemma != word:
                    query_candidates.append(word)
            
            # å¯¹äºä»¥ãç»“å°¾çš„åŠ¨è¯,æ·»åŠ ãå˜ä½“ (å¤åˆè¯å¸¸è§å½¢å¼)
            if word_lemma.endswith('ã'):
                ki_variant = word_lemma[:-1] + 'ã'
                query_candidates.append(ki_variant)
            
            # è·å–å¼ºåˆ¶è¯»éŸ³ (å¦‚æœæœ‰çš„è¯)
            forced_reading = word_to_reading.get(word)
            if forced_reading and verbose:
                print(f"      ğŸ”’ å¼ºåˆ¶è¯»éŸ³: {forced_reading}")
            
            # 1. æŸ¥è¯¢é‡Šä¹‰ (ä½¿ç”¨å€™é€‰è¯å›é€€æŸ¥è¯¢)
            definition = ""
            successful_query_form = word  # è®°å½•æˆåŠŸæŸ¥è¯¢çš„å½¢æ€,ç”¨äºæ›´æ–°å¡ç‰‡æ˜¾ç¤º
            
            if meanings_lookup:
                try:
                    # å¦‚æœæœ‰å¼ºåˆ¶è¯»éŸ³,ç”¨å‡åæŸ¥è¯¢
                    if forced_reading:
                        definition = meanings_lookup.lookup(forced_reading)
                        if definition and verbose:
                            # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«åŸæ±‰å­—å’Œå‡å
                            if word_lemma in definition or word in definition:
                                plain_def = re.sub(r'<[^>]+>', '', definition)[:100]
                                print(f"      âœ… é‡Šä¹‰ (å‡åæŸ¥è¯¢): {plain_def}...")
                            else:
                                print(f"      âš ï¸  å‡åæŸ¥è¯¢ç»“æœä¸­æœªæ‰¾åˆ°åŸè¯ '{word}',ç»“æœå¯èƒ½ä¸å‡†ç¡®")
                        elif verbose:
                            print(f"      âš ï¸  å‡å '{forced_reading}' æœªæ‰¾åˆ°é‡Šä¹‰,å°è¯•ç”¨è¯å…ƒæŸ¥è¯¢")
                            # å‡åæŸ¥è¯¢å¤±è´¥,å›é€€åˆ°è¯å…ƒæŸ¥è¯¢
                            definition = meanings_lookup.lookup(word_lemma)
                            if definition:
                                successful_query_form = word_lemma
                                plain_def = re.sub(r'<[^>]+>', '', definition)[:100]
                                print(f"      âœ… é‡Šä¹‰ (è¯å…ƒæŸ¥è¯¢): {plain_def}...")
                    else:
                        # æ²¡æœ‰å¼ºåˆ¶è¯»éŸ³,ä½¿ç”¨å€™é€‰è¯å›é€€æŸ¥è¯¢
                        # å¯¹äºç‰‡å‡åè¯æ±‡: å€™é€‰è¯åˆ—è¡¨å·²åŒ…å« [ç‰‡å‡å, å¹³å‡å, ...]
                        # å¯¹äºå…¶ä»–è¯æ±‡: å€™é€‰è¯åˆ—è¡¨åŒ…å« [è¯å…ƒ, åŸè¯, ãâ†’ãå˜ä½“]
                        for candidate in query_candidates:
                            definition = meanings_lookup.lookup(candidate)
                            if definition:
                                successful_query_form = candidate  # è®°å½•æˆåŠŸçš„æŸ¥è¯¢å½¢æ€
                                if candidate != query_candidates[0] and verbose:
                                    print(f"      ğŸ”„ ä½¿ç”¨å˜ä½“æŸ¥è¯¢: {candidate}")
                                plain_def = re.sub(r'<[^>]+>', '', definition)[:100]
                                print(f"      âœ… é‡Šä¹‰: {plain_def}...")
                                break
                    
                    if not definition and verbose:
                        print(f"      âš ï¸  æœªæ‰¾åˆ°é‡Šä¹‰")
                except Exception as e:
                    if verbose:
                        print(f"      âš ï¸  é‡Šä¹‰æŸ¥è¯¢å¤±è´¥: {e}")
                    definition = ""
            else:
                if verbose:
                    print(f"      âš ï¸  é‡Šä¹‰æŸ¥è¯¢æœªåˆå§‹åŒ–,è·³è¿‡")
            
            # 2. æŸ¥è¯¢éŸ³é¢‘å’ŒéŸ³è°ƒ (æ”¯æŒå¤šè¯»éŸ³)
            reading = ''
            pitch_pos = ''
            pitch_src = ''
            audio_src = ''
            all_readings_json = ''
            audio_result = None  # åˆå§‹åŒ– audio_result,ç¡®ä¿ä½œç”¨åŸŸæ­£ç¡®
            
            if audio_lookup:
                try:
                    # å¦‚æœæœ‰å¼ºåˆ¶è¯»éŸ³,ç›´æ¥ç”¨å‡åæŸ¥è¯¢
                    if forced_reading:
                        audio_result = audio_lookup.lookup(forced_reading, verbose=False, return_all_pitches=True)
                        if verbose and audio_result and audio_result.get('reading'):
                            print(f"      âœ… ä½¿ç”¨å¼ºåˆ¶è¯»éŸ³æŸ¥è¯¢: {forced_reading}")
                    else:
                        # æ²¡æœ‰å¼ºåˆ¶è¯»éŸ³,ä½¿ç”¨å€™é€‰è¯å›é€€æŸ¥è¯¢
                        for candidate in query_candidates:
                            audio_result = audio_lookup.lookup(candidate, verbose=False, return_all_pitches=True)
                            if audio_result and audio_result.get('reading'):
                                if candidate != word_lemma and verbose:
                                    print(f"      ğŸ”„ ä½¿ç”¨å˜ä½“æŸ¥è¯¢éŸ³é¢‘: {candidate}")
                                break
                    
                    if audio_result:
                        reading = audio_result.get('reading', '') or ''
                        pitch_pos = audio_result.get('pitch_position', '') or ''
                        pitch_src = audio_result.get('pitch_source', '') or ''
                        audio_src = audio_result.get('audio_source', '') or ''
                        
                        # è·å–æ‰€æœ‰å€™é€‰è¯»éŸ³
                        all_pitches = audio_result.get('all_pitches', [])
                        
                        all_readings_json = json.dumps(
                            [{'reading': r, 'pitch_position': p} for r, p in all_pitches],
                            ensure_ascii=False
                        ) if all_pitches else ''
                        
                        if verbose and reading:
                            plain_reading = re.sub(r'<[^>]+>', '', reading)
                            print(f"      ğŸµ è¯»éŸ³: {plain_reading} {pitch_pos}")
                            if not forced_reading and len(all_pitches) > 1:
                                print(f"      ğŸ“‹ å…± {len(all_pitches)} ä¸ªå€™é€‰è¯»éŸ³")
                        elif verbose:
                            print(f"      âš ï¸  æœªæ‰¾åˆ°éŸ³é¢‘/éŸ³è°ƒ")
                    else:
                        if verbose:
                            print(f"      âš ï¸  æœªæ‰¾åˆ°éŸ³é¢‘/éŸ³è°ƒ")
                except Exception as e:
                    if verbose:
                        print(f"      âš ï¸  éŸ³é¢‘æŸ¥è¯¢å¤±è´¥: {e}")
                    audio_result = None  # ç¡®ä¿å¼‚å¸¸æ—¶ audio_result ä¸º None
            else:
                if verbose:
                    print(f"      âš ï¸  éŸ³é¢‘æŸ¥è¯¢æœªåˆå§‹åŒ–,è·³è¿‡")
            
            # 3. æŸ¥è¯¢é¢‘ç‡ (ä½¿ç”¨å€™é€‰è¯å›é€€æŸ¥è¯¢)
            freq_str, freq_rank = None, None
            for candidate in query_candidates:
                freq_str, freq_rank = freq_index.lookup(candidate)
                if freq_str:
                    if candidate != word_lemma and verbose:
                        print(f"      ğŸ”„ ä½¿ç”¨å˜ä½“æŸ¥è¯¢é¢‘ç‡: {candidate}")
                    break
            
            if verbose:
                if freq_str:
                    print(f"      ğŸ“Š é¢‘ç‡: {freq_str} (æ’åºå€¼: {freq_rank})")
                else:
                    print(f"      âš ï¸  æœªæ‰¾åˆ°é¢‘ç‡æ•°æ®")
            
            # 4. è½¬æ¢éŸ³è°ƒç±»å‹ (ä½¿ç”¨å‡åè¯»éŸ³é•¿åº¦)
            pitch_type = pitch_position_to_type(pitch_pos, reading)
            if verbose and pitch_type:
                print(f"      ğŸ¼ å£°è°ƒç±»å‹: {pitch_type}")
            
            # 5. Base64 ç¼–ç åª’ä½“æ–‡ä»¶
            if verbose:
                print(f"      ğŸ“¦ ç¼–ç åª’ä½“æ–‡ä»¶...")
            
            sentence_audio_b64 = file_to_base64(aud_path)
            picture_b64 = file_to_base64(img_path)
            
            # è·å–å•è¯éŸ³é¢‘ (AudioLookup ç›´æ¥è¿”å› Base64 data URI)
            word_audio_b64 = ""
            if audio_result and audio_result.get('audio_base64'):
                # AudioLookup è¿”å›çš„æ˜¯çº¯ Base64 æ•°æ®,éœ€è¦è½¬æ¢ä¸º data URI
                audio_b64 = audio_result['audio_base64']
                audio_mime = audio_result.get('audio_mime', 'audio/mpeg')
                word_audio_b64 = f"data:{audio_mime};base64,{audio_b64}"
                if verbose:
                    print(f"         âœ… å•è¯éŸ³é¢‘: {audio_src}")
            
            # å¦‚æœ AudioLookup æ²¡æ‰¾åˆ°éŸ³é¢‘,å°è¯•ä» DJS_N (å¤§è¾æ³‰ç¬¬äºŒç‰ˆ) è·å– (ä½¿ç”¨å€™é€‰è¯å›é€€æŸ¥è¯¢)
            # æ³¨æ„: DJS å¯èƒ½å·²ç»åœ¨ AudioLookup ä¸­,æ‰€ä»¥è¿™ä¸ªæ˜¯çœŸæ­£çš„å¤‡é€‰
            if not word_audio_b64 and dicts_dir:
                djs_mdx = dicts_dir / "DJS_N" / "DJS.mdx"
                if djs_mdx.exists():
                    try:
                        for candidate in query_candidates:
                            audio_infos = get_all_audio_info_from_mdx(djs_mdx, candidate, "å¤§è¾æ³‰")
                            if audio_infos:
                                if candidate != word_lemma and verbose:
                                    print(f"         ğŸ”„ ä½¿ç”¨å˜ä½“æŸ¥è¯¢å¤§è¾æ³‰éŸ³é¢‘: {candidate}")
                                # ä½¿ç”¨ç¬¬ä¸€ä¸ªéŸ³é¢‘
                                first_audio = audio_infos[0]
                                if first_audio.data_uri:
                                    word_audio_b64 = first_audio.data_uri
                                    audio_src = "å¤§è¾æ³‰"
                                    if verbose:
                                        print(f"         âœ… å•è¯éŸ³é¢‘ (å¤§è¾æ³‰): {first_audio.format}")
                                break
                    except Exception as e:
                        if verbose:
                            print(f"         âš ï¸  å¤§è¾æ³‰éŸ³é¢‘æŸ¥è¯¢å¤±è´¥: {e}")
            
            # åˆ›å»ºå¡ç‰‡æ•°æ®
            # æ³¨æ„: word å­—æ®µä½¿ç”¨æˆåŠŸæŸ¥è¯¢åˆ°çš„å½¢æ€,è¿™æ ·æ˜¾ç¤ºçš„æ˜¯è¯å…¸ä¸­çœŸå®å­˜åœ¨çš„è¯æ¡
            # ä¾‹å¦‚: ç‹‚ã„å’²ã[[ç‹‚ã„å’²ã]] ä¼šæ˜¾ç¤ºä¸º "ç‹‚ã„å’²ã"
            card = CardData(
                word=successful_query_form,  # ä½¿ç”¨æˆåŠŸæŸ¥è¯¢çš„å½¢æ€è€ŒéåŸå§‹è¾“å…¥
                sentence=sent,
                sentence_furigana=furig,
                definition=definition,
                reading=reading,
                pitch_position=pitch_pos,
                pitch_type=pitch_type,
                pitch_source=pitch_src,
                sentence_audio_base64=sentence_audio_b64,
                word_audio_base64=word_audio_b64,
                word_audio_source=audio_src,
                picture_base64=picture_b64,
                bccwj_frequency=freq_str or '',
                bccwj_freq_sort=str(freq_rank) if freq_rank is not None else '',
                anime_name=anime_name,
                episode=episode,
                start_time=start,
                end_time=end,
                lemma=word_lemma,  # å­˜å‚¨è¯å…ƒå½¢å¼
                all_readings=all_readings_json
            )
            
            cards.append(card)
            if verbose:
                print()
    
    return cards


# ----------------------- è·¯å¾„å¤„ç†å·¥å…· -----------------------

def normalize_path(path_str: str) -> Path:
    """
    è§„èŒƒåŒ–è·¯å¾„å­—ç¬¦ä¸²,å¤„ç† Unicode å­—ç¬¦
    
    å¸¸è§é—®é¢˜:
    - å…¨è§’ç©ºæ ¼ (U+3000) è¢«è¯†åˆ«ä¸º \\u3000
    - æ—¥è¯­æ–‡ä»¶åç¼–ç é—®é¢˜
    """
    if not path_str:
        return None
    
    # å¤„ç† Unicode è½¬ä¹‰åºåˆ—
    try:
        # å¦‚æœå­—ç¬¦ä¸²åŒ…å« \u è½¬ä¹‰,å°è¯•è§£ç 
        if '\\u' in path_str:
            path_str = path_str.encode().decode('unicode-escape')
    except Exception:
        pass
    
    # åˆ›å»º Path å¯¹è±¡
    path = Path(path_str)
    
    # å¤„ç†å…¨è§’ç©ºæ ¼å’Œå…¶ä»–å…¨è§’å­—ç¬¦
    path_str = str(path)
    path_str = path_str.replace('\u3000', ' ')  # å…¨è§’ç©ºæ ¼ â†’ åŠè§’ç©ºæ ¼
    
    return Path(path_str)


def safe_path_from_args(arg_value) -> Optional[Path]:
    """
    å®‰å…¨åœ°ä»å‘½ä»¤è¡Œå‚æ•°è·å– Path å¯¹è±¡
    
    Args:
        arg_value: argparse è§£æçš„å‚æ•°å€¼
    
    Returns:
        è§„èŒƒåŒ–çš„ Path å¯¹è±¡,æˆ– None
    """
    if arg_value is None:
        return None
    
    if isinstance(arg_value, Path):
        # å·²ç»æ˜¯ Path,ä½†ä»éœ€è§„èŒƒåŒ–
        return normalize_path(str(arg_value))
    
    return normalize_path(str(arg_value))


# ----------------------- CSV å¯¼å‡º -----------------------

def write_csv(cards: List[CardData], csv_path: Path, outdir: Path) -> None:
    """å°†å¡ç‰‡æ•°æ®å†™å…¥ CSV (åŒ…å« Base64 ç¼–ç çš„åª’ä½“) å¹¶å¯¼å‡ºç‹¬ç«‹åª’ä½“æ–‡ä»¶
    
    ä½¿ç”¨ DataFrame ç®¡ç†æ•°æ®:
    1. ç»Ÿè®¡é‡å¤å•è¯å‡ºç°æ¬¡æ•°
    2. å»é‡ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„æ•°æ®
    3. æ·»åŠ  duplicate_count å­—æ®µæ ‡è®°é‡å¤æ¬¡æ•°
    """
    if not pd:
        print("\nâŒ pandas æœªå®‰è£…,æ— æ³•å¯¼å‡º CSV")
        return
    
    ensure_dir(csv_path.parent)
    
    # åˆ›å»ºåª’ä½“æ–‡ä»¶ç›®å½•
    media_dir = outdir / "media"
    ensure_dir(media_dir)
    
    print(f"\nğŸ“Š ä½¿ç”¨ DataFrame å¤„ç†æ•°æ®...")
    
    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame([asdict(card) for card in cards])
    
    print(f"   åŸå§‹å¡ç‰‡æ•°: {len(df)}")
    
    # ç»Ÿè®¡æ¯ä¸ªå•è¯çš„é‡å¤æ¬¡æ•°
    word_counts = df['word'].value_counts().to_dict()
    df['duplicate_count'] = df['word'].map(word_counts)
    
    # æ‰¾å‡ºé‡å¤çš„å•è¯
    duplicates = df[df['duplicate_count'] > 1]['word'].unique()
    if len(duplicates) > 0:
        print(f"   å‘ç°é‡å¤å•è¯: {len(duplicates)} ä¸ª")
        print(f"   ç¤ºä¾‹: {', '.join(list(duplicates)[:5])}")
    
    # å»é‡ (ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°)
    df_dedup = df.drop_duplicates(subset=['word'], keep='first').copy()
    removed = len(df) - len(df_dedup)
    
    if removed > 0:
        print(f"   å»é‡åå¡ç‰‡æ•°: {len(df_dedup)} (ç§»é™¤ {removed} å¼ é‡å¤)")
    
    # å…ˆå¯¼å‡ºåª’ä½“æ–‡ä»¶,è·å–æ–‡ä»¶è·¯å¾„
    print(f"\nğŸ“¦ å¯¼å‡ºç‹¬ç«‹åª’ä½“æ–‡ä»¶åˆ°: {media_dir}")
    
    media_stats = {
        'pictures': 0,
        'sentence_audio': 0,
        'word_audio': 0
    }
    
    # æ·»åŠ åª’ä½“æ–‡ä»¶è·¯å¾„åˆ—
    df_dedup['picture_file'] = ''
    df_dedup['sentence_audio_file'] = ''
    df_dedup['word_audio_file'] = ''
    
    # éå†å»é‡åçš„æ•°æ®,å¯¼å‡ºåª’ä½“æ–‡ä»¶å¹¶è®°å½•è·¯å¾„
    for idx in df_dedup.index:
        i = idx + 1
        card_word = df_dedup.at[idx, 'word']
        
        # å¯¼å‡ºå›¾ç‰‡ (JPG æ ¼å¼)
        picture_b64 = df_dedup.at[idx, 'picture_base64']
        if picture_b64:
            try:
                if picture_b64.startswith('data:'):
                    header, b64_data = picture_b64.split(';base64,', 1)
                    import base64
                    img_data = base64.b64decode(b64_data)
                    
                    # åˆ¤æ–­åŸå§‹æ ¼å¼,ç»Ÿä¸€ä¿å­˜ä¸º JPG
                    img_filename = f"{card_word}_{i}_pic.jpg"
                    img_path = media_dir / img_filename
                    with open(img_path, 'wb') as f:
                        f.write(img_data)
                    
                    # è®°å½•ç›¸å¯¹è·¯å¾„
                    df_dedup.at[idx, 'picture_file'] = f"media/{img_filename}"
                    media_stats['pictures'] += 1
            except Exception as e:
                print(f"   âš ï¸  å¯¼å‡ºå›¾ç‰‡å¤±è´¥ ({card_word}): {e}")
        
        # å¯¼å‡ºå¥å­éŸ³é¢‘
        sentence_audio_b64 = df_dedup.at[idx, 'sentence_audio_base64']
        if sentence_audio_b64:
            try:
                if sentence_audio_b64.startswith('data:'):
                    header, b64_data = sentence_audio_b64.split(';base64,', 1)
                    mime = header.split(':')[1]
                    ext = 'mp3' if 'mpeg' in mime else 'mp4' if 'mp4' in mime else mime.split('/')[-1]
                    
                    import base64
                    audio_data = base64.b64decode(b64_data)
                    
                    audio_filename = f"{card_word}_{i}_sent.{ext}"
                    audio_path = media_dir / audio_filename
                    with open(audio_path, 'wb') as f:
                        f.write(audio_data)
                    
                    df_dedup.at[idx, 'sentence_audio_file'] = f"media/{audio_filename}"
                    media_stats['sentence_audio'] += 1
            except Exception as e:
                print(f"   âš ï¸  å¯¼å‡ºå¥å­éŸ³é¢‘å¤±è´¥ ({card_word}): {e}")
        
        # å¯¼å‡ºå•è¯éŸ³é¢‘
        word_audio_b64 = df_dedup.at[idx, 'word_audio_base64']
        if word_audio_b64:
            try:
                if word_audio_b64.startswith('data:'):
                    header, b64_data = word_audio_b64.split(';base64,', 1)
                    mime = header.split(':')[1]
                    ext = 'mp3' if 'mpeg' in mime else 'aac' if 'aac' in mime else mime.split('/')[-1]
                    
                    import base64
                    audio_data = base64.b64decode(b64_data)
                    
                    audio_filename = f"{card_word}_{i}_word.{ext}"
                    audio_path = media_dir / audio_filename
                    with open(audio_path, 'wb') as f:
                        f.write(audio_data)
                    
                    df_dedup.at[idx, 'word_audio_file'] = f"media/{audio_filename}"
                    media_stats['word_audio'] += 1
            except Exception as e:
                print(f"   âš ï¸  å¯¼å‡ºå•è¯éŸ³é¢‘å¤±è´¥ ({card_word}): {e}")
    
    print(f"   âœ… å›¾ç‰‡: {media_stats['pictures']} ä¸ª")
    print(f"   âœ… å¥å­éŸ³é¢‘: {media_stats['sentence_audio']} ä¸ª")
    print(f"   âœ… å•è¯éŸ³é¢‘: {media_stats['word_audio']} ä¸ª")
    
    # å®šä¹‰ CSV å­—æ®µé¡ºåº (ä¸åŒ…å« Base64 æ•°æ®,åªåŒ…å«æ–‡ä»¶è·¯å¾„)
    csv_fields = [
        'word', 'duplicate_count', 'sentence', 'sentence_furigana', 'definition',
        'reading', 'pitch_position', 'pitch_type', 'pitch_source',
        'sentence_audio_file', 'word_audio_file', 'word_audio_source',
        'picture_file',
        'bccwj_frequency', 'bccwj_freq_sort', 
        'anime_name', 'episode',
        'start_time', 'end_time',
        'lemma', 'all_readings'
    ]

    # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
    for field in csv_fields:
        if field not in df_dedup.columns:
            df_dedup[field] = ''
    
    try:
        # å¯¼å‡º CSV (åªåŒ…å«æ–‡æœ¬å’Œæ–‡ä»¶è·¯å¾„)
        df_dedup[csv_fields].to_csv(csv_path, index=False, encoding='utf-8')
        
        print(f"\nâœ… CSV å·²ç”Ÿæˆ: {csv_path}")
        print(f"   æ€»å¡ç‰‡æ•°: {len(df_dedup)}")
        print(f"   â„¹ï¸  CSV åŒ…å«åª’ä½“æ–‡ä»¶è·¯å¾„,ä¸å« Base64 æ•°æ®")
        
    except Exception as e:
        print(f"\nâŒ CSV å†™å…¥å¤±è´¥: {e}")
        raise


# ----------------------- Anki æ¨é€ -----------------------

def push_to_anki(
    cards: List[CardData],
    anki: AnkiConnect,
    deck_name: str,
    model_name: str,
    tags: List[str],
    allow_duplicates: bool = False,
    verbose: bool = True
) -> Tuple[int, int]:
    """
    å°†å¡ç‰‡æ¨é€åˆ° Anki
    
    Args:
        cards: å¡ç‰‡æ•°æ®åˆ—è¡¨
        anki: AnkiConnect å®ä¾‹
        deck_name: ç‰Œç»„åç§°
        model_name: ç¬”è®°ç±»å‹
        tags: æ ‡ç­¾åˆ—è¡¨
        allow_duplicates: æ˜¯å¦å…è®¸é‡å¤
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        (æˆåŠŸæ•°, å¤±è´¥æ•°)
    """
    print(f"\nğŸš€ å¼€å§‹æ¨é€åˆ° Anki...")
    print(f"   ç‰Œç»„: {deck_name}")
    print(f"   ç¬”è®°ç±»å‹: {model_name}")
    print(f"   æ ‡ç­¾: {', '.join(tags)}")
    print(f"   å¡ç‰‡æ•°: {len(cards)}")
    print()
    
    success_count = 0
    error_count = 0
    word_counter = {}  # ç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    
    for idx, card in enumerate(cards, 1):
        word = card.word
        word_counter[word] = word_counter.get(word, 0) + 1
        card_index = word_counter[word]
        
        try:
            # 1. å‡†å¤‡åª’ä½“æ–‡ä»¶
            picture_filename = ""
            word_audio_filename = ""
            sentence_audio_filename = ""
            
            # å›¾ç‰‡
            if card.picture_base64:
                try:
                    # æå– Base64 æ•°æ®
                    if card.picture_base64.startswith('data:'):
                        _, b64_data = card.picture_base64.split(';base64,', 1)
                    else:
                        b64_data = card.picture_base64
                    
                    picture_filename = f"{word}_{card_index}_pic.jpg"
                    anki.store_media_file(picture_filename, b64_data)
                except Exception as e:
                    if verbose:
                        print(f"   âš ï¸  [{idx}/{len(cards)}] {word}: å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {e}")
            
            # å•è¯éŸ³é¢‘
            if card.word_audio_base64:
                try:
                    if card.word_audio_base64.startswith('data:'):
                        header, b64_data = card.word_audio_base64.split(';base64,', 1)
                        # ä» MIME ç±»å‹æ¨æ–­æ‰©å±•å
                        mime = header.split(':')[1]
                        ext = 'mp3' if 'mpeg' in mime else 'aac' if 'aac' in mime else 'mp3'
                    else:
                        b64_data = card.word_audio_base64
                        ext = 'mp3'
                    
                    word_audio_filename = f"{word}_{card_index}_word.{ext}"
                    anki.store_media_file(word_audio_filename, b64_data)
                except Exception as e:
                    if verbose:
                        print(f"   âš ï¸  [{idx}/{len(cards)}] {word}: å•è¯éŸ³é¢‘ä¸Šä¼ å¤±è´¥: {e}")
            
            # å¥å­éŸ³é¢‘
            if card.sentence_audio_base64:
                try:
                    if card.sentence_audio_base64.startswith('data:'):
                        header, b64_data = card.sentence_audio_base64.split(';base64,', 1)
                        mime = header.split(':')[1]
                        ext = 'mp3' if 'mpeg' in mime else 'mp4' if 'mp4' in mime else 'm4a'
                    else:
                        b64_data = card.sentence_audio_base64
                        ext = 'm4a'
                    
                    sentence_audio_filename = f"{word}_{card_index}_sent.{ext}"
                    anki.store_media_file(sentence_audio_filename, b64_data)
                except Exception as e:
                    if verbose:
                        print(f"   âš ï¸  [{idx}/{len(cards)}] {word}: å¥å­éŸ³é¢‘ä¸Šä¼ å¤±è´¥: {e}")
            
            # 2. å‡†å¤‡å­—æ®µ
            # é«˜äº®å•è¯
            sentence_html = card.sentence
            if word in sentence_html:
                sentence_html = sentence_html.replace(word, f'<span class="highlight">{word}</span>')
            
            # è¯»éŸ³æ ¼å¼åŒ–ä¸º HTML åˆ—è¡¨ (å¸¦éŸ³è°ƒæ ‡è®°)
            reading_html = ''
            if card.reading:
                all_readings = []
                if card.all_readings:
                    try:
                        readings_list = json.loads(card.all_readings)
                        all_readings = readings_list  # ä¿ç•™å®Œæ•´ä¿¡æ¯
                    except:
                        # é™çº§: åªæœ‰åŸºç¡€è¯»éŸ³
                        all_readings = [{'reading': card.reading, 'pitch_position': card.pitch_position}]
                else:
                    all_readings = [{'reading': card.reading, 'pitch_position': card.pitch_position}]
                
                reading_html = '<ol>'
                for r_info in all_readings:
                    # æå–è¯»éŸ³å’ŒéŸ³è°ƒä¿¡æ¯
                    if isinstance(r_info, dict):
                        r_reading = r_info.get('reading', '')
                        r_pitch = r_info.get('pitch_position', '')
                    else:
                        # å­—ç¬¦ä¸²æ ¼å¼(é™çº§å¤„ç†)
                        r_reading = str(r_info)
                        r_pitch = card.pitch_position
                    
                    # å»é™¤ HTML æ ‡ç­¾,è·å–çº¯å‡å
                    clean_reading = re.sub(r'<[^>]+>', '', r_reading)
                    
                    # å¤„ç†é•¿éŸ³ç¬¦: æ ¹æ®åŸè¯ç±»å‹å†³å®šå¦‚ä½•å¤„ç†
                    if is_all_katakana(word):
                        # åŸè¯æ˜¯å…¨ç‰‡å‡å (å¦‚ ã‚³ãƒ¼ãƒ’ãƒ¼): ä¿æŒåŸæ ·,ä¸è½¬æ¢
                        # clean_reading ä¿æŒåŸæ¥çš„ç‰‡å‡åæˆ–å¹³å‡å
                        pass
                    else:
                        # åŸè¯ä¸æ˜¯å…¨ç‰‡å‡å (å¦‚ ç²¾éœŠ, å­¦æ ¡): è½¬ä¸ºå¹³å‡åå¹¶å±•å¼€é•¿éŸ³
                        clean_reading = katakana_to_hiragana(clean_reading)
                        clean_reading = expand_long_vowel(clean_reading)
                    
                    # æå–éŸ³è°ƒä½ç½®æ•°å­—
                    pitch_num = None
                    if r_pitch:
                        match = re.search(r'\[?(\d+)\]?', str(r_pitch))
                        if match:
                            pitch_num = int(match.group(1))
                    
                    # ç”Ÿæˆå¸¦éŸ³è°ƒæ ‡è®°çš„ HTML
                    if pitch_num is not None and clean_reading:
                        # æ ¹æ®éŸ³è°ƒä½ç½®åˆ¤æ–­ç±»å‹
                        r_pitch_type = pitch_position_to_type(f"[{pitch_num}]", clean_reading)
                        pitch_html = generate_pitch_html(clean_reading, pitch_num, r_pitch_type)
                        reading_html += f'<li>{pitch_html}</li>'
                    else:
                        # æ— éŸ³è°ƒä¿¡æ¯,ç›´æ¥æ˜¾ç¤ºè¯»éŸ³
                        reading_html += f'<li>{r_reading}</li>'
                
                reading_html += '</ol>'
            
            # éŸ³è°ƒä½ç½®æ ¼å¼åŒ–
            pitch_position_html = ''
            if card.pitch_position:
                pos_num = card.pitch_position.strip('[]')
                pitch_position_html = f'<ol><li><span style="display:inline;"><span>[</span><span>{pos_num}</span><span>]</span></span></li></ol>'
            
            # é¢‘ç‡æ ¼å¼åŒ–
            frequency_html = ''
            if card.bccwj_frequency:
                frequency_html = f'<ul style="text-align: left;"><li>BCCWJ: {card.bccwj_frequency}</li></ul>'
            
            # miscInfo: åŠ¨æ¼«å|é›†æ•°|æ—¶é—´æˆ³
            misc_info_parts = []
            if card.anime_name:
                misc_info_parts.append(card.anime_name)
            if card.episode:
                misc_info_parts.append(card.episode)
            if card.start_time:
                time_str = format_time_hhmmss(card.start_time)
                misc_info_parts.append(time_str)
            misc_info = ' | '.join(misc_info_parts)
            
            # 3. æ„å»º Anki ç¬”è®°
            fields = {
                'word': word,
                'sentence': sentence_html,
                'sentenceFurigana': card.sentence_furigana,
                'sentenceEng': '',
                'reading': reading_html,
                'sentenceCard': '',
                'audioCard': '',
                'notes': '',
                'picture': f'<img src="{picture_filename}" />' if picture_filename else '',
                'wordAudio': f'[sound:{word_audio_filename}]' if word_audio_filename else '',
                'sentenceAudio': f'[sound:{sentence_audio_filename}]' if sentence_audio_filename else '',
                'selectionText': '',
                'definition': card.definition,
                'glossary': card.definition,
                'pitchPosition': pitch_position_html,
                'pitch': card.pitch_type,
                'frequency': frequency_html,
                'freqSort': card.bccwj_freq_sort,
                'miscInfo': misc_info,
                'dictionaryPreference': ''
            }
            
            note = {
                'deckName': deck_name,
                'modelName': model_name,
                'fields': fields,
                'tags': tags,
                'options': {
                    'allowDuplicate': allow_duplicates,
                    'duplicateScope': 'collection'
                }
            }
            
            # 4. æ·»åŠ åˆ° Anki
            note_id = anki.add_note(note)
            
            if verbose:
                print(f"   âœ… [{idx}/{len(cards)}] {word} (ID: {note_id})")
            
            success_count += 1
            
        except Exception as e:
            if verbose:
                print(f"   âŒ [{idx}/{len(cards)}] {word}: {e}")
            error_count += 1
    
    return success_count, error_count


# ----------------------- ä¸»ç¨‹åº -----------------------

def main():
    ap = argparse.ArgumentParser(
        description='JP Media Mining â†’ Anki Cards (é‡æ„ç‰ˆ)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python jp_media_mining_refactored.py \\
    --video "episodes/Ep01.mp4" \\
    --subs  "subs/Ep01.ja.srt" \\
    --words "unknown/Ep01_words.txt" \\
    --primary-mdx   "dicts/primary" \\
    --secondary-mdx "dicts/secondary" \\
    --tertiary-mdx  "dicts/tertiary" \\
    --nhk-old "dicts/NHK_Old" \\
    --nhk-new "dicts/NHK_New" \\
    --djs     "dicts/DJS" \\
    --outdir  "out/Ep01" \\
    --csv     "out/Ep01/cards.csv"
        """
    )
    
    # å¿…éœ€å‚æ•°
    ap.add_argument('--video', required=True, type=Path, help='è§†é¢‘æ–‡ä»¶è·¯å¾„')
    ap.add_argument('--subs', required=True, type=Path, help='å­—å¹•æ–‡ä»¶è·¯å¾„ (.srt/.ass/.vtt)')
    ap.add_argument('--words', required=True, type=Path, help='ç›®æ ‡å•è¯åˆ—è¡¨æ–‡ä»¶')
    ap.add_argument('--outdir', required=True, type=Path, help='è¾“å‡ºç›®å½•')
    
    # MDX è¯å…¸ (é‡Šä¹‰)
    ap.add_argument('--primary-mdx', type=Path, 
                   help='ä¸»è¦é‡Šä¹‰è¯å…¸ç›®å½• (åŒ…å« .mdx æ–‡ä»¶)')
    ap.add_argument('--secondary-mdx', type=Path,
                   help='æ¬¡è¦é‡Šä¹‰è¯å…¸ç›®å½•')
    ap.add_argument('--tertiary-mdx', type=Path,
                   help='ç¬¬ä¸‰çº§é‡Šä¹‰è¯å…¸ç›®å½• (ä»…å½“ secondary æ— ç»“æœæ—¶æŸ¥è¯¢)')
    ap.add_argument('--use-jamdict', action='store_true',
                   help='å¯ç”¨ JMDict ä½œä¸ºæœ€åçš„ fallback')
    
    # MDX è¯å…¸ (éŸ³é¢‘å’ŒéŸ³è°ƒ)
    ap.add_argument('--nhk-old', type=Path,
                   help='NHK æ—§ç‰ˆè¯å…¸ç›®å½• (éŸ³è°ƒä¿¡æ¯)')
    ap.add_argument('--nhk-new', type=Path,
                   help='NHK æ–°ç‰ˆè¯å…¸ç›®å½• (éŸ³é¢‘)')
    ap.add_argument('--djs', type=Path,
                   help='å¤§è¾æ³‰è¯å…¸ç›®å½• (éŸ³é¢‘)')
    
    # é¢‘ç‡æ•°æ®
    ap.add_argument('--freq', type=Path,
                   help='é¢‘ç‡æ•°æ®æ–‡ä»¶ (JSON/ZIP/CSV/TSV), å¦‚ BCCWJ term_meta_bank_1.json')
    
    # åª’ä½“å¤„ç†é€‰é¡¹
    ap.add_argument('--pad', type=float, default=0.0,
                   help='éŸ³é¢‘è£å‰ªå‰åå¡«å……æ—¶é—´(ç§’), é»˜è®¤ 0')
    ap.add_argument('--vf', type=str, default=None,
                   help='FFmpeg è§†é¢‘æ»¤é•œ, å¦‚ "scale=1280:-1"')
    
    # è¾“å‡ºé€‰é¡¹
    ap.add_argument('--csv', type=Path, required=True,
                   help='CSV è¾“å‡ºè·¯å¾„')
    ap.add_argument('--quiet', action='store_true',
                   help='å®‰é™æ¨¡å¼,å‡å°‘è¾“å‡º')
    
    # Anki æ¨é€é€‰é¡¹
    ap.add_argument('--anki', action='store_true',
                   help='æ¨é€åˆ° Anki (éœ€è¦ AnkiConnect æ’ä»¶)')
    ap.add_argument('--anki-deck', type=str, default='Japanese::Anime',
                   help='Anki ç‰Œç»„åç§° (é»˜è®¤: Japanese::Anime)')
    ap.add_argument('--anki-model', type=str, default='Senren',
                   help='Anki ç¬”è®°ç±»å‹ (é»˜è®¤: Senren)')
    ap.add_argument('--anki-tags', type=str, nargs='+', default=['anime', 'mining'],
                   help='Anki å¡ç‰‡æ ‡ç­¾ (é»˜è®¤: anime mining)')
    ap.add_argument('--anki-allow-duplicates', action='store_true',
                   help='å…è®¸é‡å¤å¡ç‰‡')
    ap.add_argument('--ankiconnect-url', type=str, default='http://localhost:8765',
                   help='AnkiConnect API åœ°å€ (é»˜è®¤: http://localhost:8765)')
    
    args = ap.parse_args()
    
    # ==================== è§„èŒƒåŒ–è·¯å¾„å‚æ•° ====================
    
    print("=" * 60)
    print("JP Media Mining (é‡æ„ç‰ˆ)")
    print("=" * 60)
    
    print("\nğŸ”§ è§„èŒƒåŒ–è·¯å¾„å‚æ•°...")
    
    # è½¬æ¢æ‰€æœ‰è·¯å¾„å‚æ•°
    video_path = safe_path_from_args(args.video)
    subs_path = safe_path_from_args(args.subs)
    words_path = safe_path_from_args(args.words)
    outdir_path = safe_path_from_args(args.outdir)
    csv_path = safe_path_from_args(args.csv)
    
    primary_mdx_path = safe_path_from_args(args.primary_mdx)
    secondary_mdx_path = safe_path_from_args(args.secondary_mdx)
    tertiary_mdx_path = safe_path_from_args(args.tertiary_mdx)
    nhk_old_path = safe_path_from_args(args.nhk_old)
    nhk_new_path = safe_path_from_args(args.nhk_new)
    djs_path = safe_path_from_args(args.djs)
    freq_path = safe_path_from_args(args.freq)
    
    print(f"   âœ… è·¯å¾„è§„èŒƒåŒ–å®Œæˆ")
    
    # æ¨å¯¼è¯å…¸æ ¹ç›®å½• (ç”¨äºæŸ¥æ‰¾ DJS_N)
    dicts_dir = None
    if djs_path and djs_path.exists():
        # djs_path é€šå¸¸æ˜¯ .../dicts/DJS/DJS.mdx
        # æˆ‘ä»¬éœ€è¦è·å– .../dicts
        dicts_dir = djs_path.parent.parent
        if not dicts_dir.is_dir():
            dicts_dir = None
    
    # ==================== åˆå§‹åŒ– ====================
    
    # 1. åŠ è½½ Fugashi åˆ†è¯å™¨
    print("\nğŸ“š åˆå§‹åŒ–åˆ†è¯å™¨...")
    try:
        tagger = Tagger()
        print("   âœ… Fugashi (UniDic) å·²åŠ è½½")
    except Exception as e:
        print(f"   âŒ Fugashi åˆå§‹åŒ–å¤±è´¥: {e}")
        return 1
    
    # 2. åŠ è½½å­—å¹•
    print(f"\nğŸ“„ åŠ è½½å­—å¹•: {subs_path.name}")
    try:
        subs = pysubs2.load(str(subs_path))
        print(f"   âœ… å…± {len(subs)} è¡Œå­—å¹•")
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # 3. åŠ è½½ç›®æ ‡å•è¯
    print(f"\nğŸ“ åŠ è½½ç›®æ ‡å•è¯: {words_path.name}")
    try:
        words = load_words(words_path)
        print(f"   âœ… å…± {len(words)} ä¸ªå•è¯")
        if not args.quiet and len(words) <= 100:
            # æ˜¾ç¤ºæ ¼å¼: word, word(reading), word[lookup_form], word(reading)[lookup_form]
            word_list_display = []
            for w, r, l in words[:100]:
                parts = [w]
                if r:
                    parts.append(f"({r})")
                if l:
                    parts.append(f"[{l}]")
                word_list_display.append(''.join(parts))
            print(f"   å•è¯åˆ—è¡¨: {', '.join(word_list_display)}")
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # 3.5 æå–åŠ¨æ¼«åå’Œé›†æ•°ä¿¡æ¯
    print(f"\nğŸ“º æå–åª’ä½“ä¿¡æ¯...")
    anime_name, episode = extract_episode_info(video_path, words_path)
    print(f"   åŠ¨æ¼«å: {anime_name}")
    print(f"   é›†æ•°: {episode}")
    
    # 4. åˆå§‹åŒ–é‡Šä¹‰æŸ¥è¯¢
    print(f"\nğŸ“– åˆå§‹åŒ–é‡Šä¹‰æŸ¥è¯¢ (MeaningsLookup)...")
    print(f"   ğŸ“‚ Primary MDX: {primary_mdx_path}")
    print(f"   ğŸ“‚ Secondary MDX: {secondary_mdx_path}")
    print(f"   ğŸ“‚ Tertiary MDX: {tertiary_mdx_path}")
    try:
        meanings_lookup = MeaningsLookup.from_dirs(
            primary_dir=primary_mdx_path,
            secondary_dir=secondary_mdx_path,
            tertiary_dir=tertiary_mdx_path,
            use_jamdict=args.use_jamdict
        )
        if meanings_lookup and meanings_lookup.all_dicts:
            print(f"   âœ… åŠ è½½è¯å…¸: {len(meanings_lookup.all_dicts)} ä¸ª")
            
            # æ˜¾ç¤ºå„çº§è¯å…¸
            if meanings_lookup.primary_dicts:
                print(f"      ğŸ“˜ Primary: {len(meanings_lookup.primary_dicts)} ä¸ª")
                for mdx_path, display_name in meanings_lookup.primary_dicts[:3]:
                    print(f"         - {display_name}")
                if len(meanings_lookup.primary_dicts) > 3:
                    print(f"         ... è¿˜æœ‰ {len(meanings_lookup.primary_dicts) - 3} ä¸ª")
            
            if meanings_lookup.secondary_dicts:
                print(f"      ğŸ“™ Secondary: {len(meanings_lookup.secondary_dicts)} ä¸ª")
                for mdx_path, display_name in meanings_lookup.secondary_dicts[:3]:
                    print(f"         - {display_name}")
                if len(meanings_lookup.secondary_dicts) > 3:
                    print(f"         ... è¿˜æœ‰ {len(meanings_lookup.secondary_dicts) - 3} ä¸ª")
            
            if meanings_lookup.tertiary_dicts:
                print(f"      ğŸ“— Tertiary: {len(meanings_lookup.tertiary_dicts)} ä¸ª")
                for mdx_path, display_name in meanings_lookup.tertiary_dicts[:3]:
                    print(f"         - {display_name}")
                if len(meanings_lookup.tertiary_dicts) > 3:
                    print(f"         ... è¿˜æœ‰ {len(meanings_lookup.tertiary_dicts) - 3} ä¸ª")
        else:
            print(f"   âš ï¸  æœªåŠ è½½ä»»ä½•è¯å…¸")
        
        if args.use_jamdict:
            print(f"   âœ… JMDict Fallback: å·²å¯ç”¨")
    except Exception as e:
        print(f"   âš ï¸  åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        meanings_lookup = None
    
    # 5. åˆå§‹åŒ–éŸ³é¢‘æŸ¥è¯¢
    print(f"\nğŸµ åˆå§‹åŒ–éŸ³é¢‘æŸ¥è¯¢ (AudioLookup)...")
    try:
        audio_lookup = AudioLookup.from_dirs(
            nhk_old_dir=nhk_old_path,
            nhk_new_dir=nhk_new_path,
            djs_dir=djs_path
        )
        audio_count = len(audio_lookup.audio_dicts) if audio_lookup.audio_dicts else 0
        print(f"   âœ… éŸ³é¢‘è¯å…¸: {audio_count} ä¸ª")
        if audio_lookup.pitch_dict:
            print(f"   âœ… éŸ³è°ƒè¯å…¸: {audio_lookup.pitch_dict.name}")
    except Exception as e:
        print(f"   âš ï¸  åˆå§‹åŒ–å¤±è´¥: {e}")
        audio_lookup = None
    
    # 6. åŠ è½½é¢‘ç‡æ•°æ®
    print(f"\nğŸ“Š åŠ è½½é¢‘ç‡æ•°æ®...")
    try:
        freq_index = FrequencyIndex(freq_path)
        if freq_index.idx:
            print(f"   âœ… å·²åŠ è½½ {len(freq_index.idx)} æ¡é¢‘ç‡æ•°æ®")
        else:
            print(f"   â„¹ï¸  æœªåŠ è½½é¢‘ç‡æ•°æ®")
    except Exception as e:
        print(f"   âš ï¸  åŠ è½½å¤±è´¥: {e}")
        freq_index = FrequencyIndex()
    
    # ==================== å¤„ç† ====================
    
    print("\n" + "=" * 60)
    print("å¼€å§‹å¤„ç†")
    print("=" * 60)
    
    try:
        cards = find_hits(
            words=words,
            subs=subs,
            tagger=tagger,
            video=video_path,
            outdir=outdir_path,
            meanings_lookup=meanings_lookup,
            audio_lookup=audio_lookup,
            freq_index=freq_index,
            anime_name=anime_name,
            episode=episode,
            dicts_dir=dicts_dir,
            pad=args.pad,
            vf=args.vf,
            verbose=not args.quiet
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\n\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # ==================== è¾“å‡º ====================
    
    if not cards:
        print("\nâš ï¸  æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„å•è¯")
        return 0
    
    print("\n" + "=" * 60)
    print("å¯¼å‡ºç»“æœ")
    print("=" * 60)
    
    # å¯¼å‡º CSV
    try:
        write_csv(cards, csv_path, outdir_path)
    except Exception as e:
        print(f"\nâŒ CSV å¯¼å‡ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # æ¨é€åˆ° Anki (å¦‚æœå¯ç”¨)
    if args.anki:
        print("\n" + "=" * 60)
        print("æ¨é€åˆ° Anki")
        print("=" * 60)
        
        try:
            # è¿æ¥ AnkiConnect
            print(f"\nğŸ”Œ è¿æ¥ AnkiConnect ({args.ankiconnect_url})...")
            anki = AnkiConnect(args.ankiconnect_url)
            
            if not anki.check_connection():
                print("\nâš ï¸  æ— æ³•è¿æ¥åˆ° Anki,è·³è¿‡æ¨é€")
                print("   è¯·ç¡®ä¿:")
                print("   1. Anki æ­£åœ¨è¿è¡Œ")
                print("   2. å·²å®‰è£… AnkiConnect æ’ä»¶ (ä»£ç : 2055492159)")
                print("   3. AnkiConnect è®¾ç½®æ­£ç¡®")
            else:
                # åˆ›å»ºç‰Œç»„
                print(f"\nğŸ“š å‡†å¤‡ç‰Œç»„: {args.anki_deck}")
                try:
                    anki.create_deck(args.anki_deck)
                    print(f"   âœ… ç‰Œç»„å·²å°±ç»ª")
                except Exception as e:
                    print(f"   âš ï¸  åˆ›å»ºç‰Œç»„å¤±è´¥: {e}")
                
                # æ¨é€å¡ç‰‡
                success, failed = push_to_anki(
                    cards=cards,
                    anki=anki,
                    deck_name=args.anki_deck,
                    model_name=args.anki_model,
                    tags=args.anki_tags,
                    allow_duplicates=args.anki_allow_duplicates,
                    verbose=not args.quiet
                )
                
                print(f"\nğŸ“Š æ¨é€ç»Ÿè®¡:")
                print(f"   æˆåŠŸ: {success} å¼ ")
                print(f"   å¤±è´¥: {failed} å¼ ")
                print(f"   æ€»è®¡: {len(cards)} å¼ ")
                
                if success > 0:
                    print(f"\nâœ… å¡ç‰‡å·²æ·»åŠ åˆ° Anki ç‰Œç»„: {args.anki_deck}")
        
        except Exception as e:
            print(f"\nâŒ Anki æ¨é€å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆ!")
    print("=" * 60)
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   å¤„ç†å­—å¹•: {len(subs)} è¡Œ")
    print(f"   ç›®æ ‡å•è¯: {len(words)} ä¸ª")
    print(f"   ç”Ÿæˆå¡ç‰‡: {len(cards)} å¼ ")
    print(f"   è¾“å‡ºç›®å½•: {outdir_path}")
    print(f"   CSV æ–‡ä»¶: {csv_path}")
    if args.anki:
        print(f"   Anki ç‰Œç»„: {args.anki_deck}")
    
    return 0


if __name__ == '__main__':
    exit(main())
